\chapter{A quick introduction}

Through-out the chapter the simple coin-flip example is used where the coin flips are assumed to have a Bernoulli distribution with an unknown probability of flipping a heads. 
Rather than providing the mathematical detail, the chapter focuses on the intuition involved in setting up a Bayesian analysis and interpreting the results. 

The reader is expected to have a solid understanding of probabilities including marginal, joint, and conditional probabilities, i.e. the material through chapter 4 of \cite{casella2002statistical}. 
To understand the remainder of the book an understanding of random samples and data reduction will be necessary, i.e. chapters 5-6 of \citep{casella2002statistical}.
To compare Bayesian approaches to non-Bayesian approaches, an understanding of both is required. 
The expectation is that the reader will learn the Bayesian approach by reading this text, but a understanding point and interval estimation, hypothesis testing, and prediction from non-Bayesian perspectives will be necessary, i.e. chapters 7-10 of \citep{casella2002statistical}.
Finally, exposure to modeling including ANOVA and regression will be extremely useful, i.e. chapters 11-12 of \citep{casella2002statistical}.


\section{Bayes' rule}

Bayes' rule as usually presented is 
\begin{equation}
P(A|B) = \frac{P(B|A)P(A)}{P(B)} 
\end{equation}

where $A$ and $B$ are events.

As an example, consider a pregnant woman has a screening test for Down syndrome and the test is positive. The woman will likely wonder what the probability is that the baby will have Down syndrome. The relevant quantities of interest are the test's sensitivity and specificity which are 0.94 and 0.77 respectively, as well as the prevalence of Down syndrome in the U.S. which is 0.001. The prevalence is the probability the baby will have Down syndrome before we knew the results of the test. 

\begin{example}

Let $D$ indicate a child with Down syndrome and $D^c$ the opposite. Let `+' indicate a positive test result and `-' a negative result. Using this notation, we have $P(+|D) = 0.94$, $P(-|D^c) = 0.77$,  and $P(D) = 0.001.$ 

Using Bayes' rule and quantities, we can update the probability to 

\begin{align*}
P(D|+) &= \frac{P(+|D)P(D)}{P(+)} \\
&= \frac{P(+|D)P(D)}{P(+|D)P(D)+P(+|D^c)P(D^c)} \\
&= \frac{0.94\cdot 0.001}{0.94\cdot 0.001+0.23\cdot 0.999} \\
&\approx 1/250 
\end{align*}

So \emph{if the baby had a 1/1000 chance of having Down syndrome without knowledge of the test}, then the baby's probability is now 1/250. The emphasis is added here because the probability after the test depends on the probability before the test.\footnote{The probability also depends on the sensitivity and specificity of the test, but for now we are assuming these are known.} If, for example, the mother is over 35 (a risk factor for Down syndrome), the prevalence in the general population is not appropriate. Instead, we would need the prevalence in the population for those mothers over 35.                
\end{example}                  

%Similarly, we could calculate the probability of having Down syndrome if the test was negative, i.e. $P(D|-)\approx 1/10,000$.

\section{Bayesian statistics}

Bayes' rule is typically introduced as applying to events, but the mathematics are the same for probability mass and density functions. So, if $\theta$ and $y$ are random variables, then the following Bayes' rule applies

\begin{equation}
p(\theta|y) = \frac{p(y|\theta)p(\theta)}{p(y)} =  \frac{p(y|\theta)p(\theta)}{\int p(y|\theta)p(\theta) d \theta} \label{eqn:bayesrule}
\end{equation}

where the probability mass or density functions are determined by their arguments. The integral is understood to be an integral if $\theta$ is continuous or a summation if $\theta$ is discrete. 

\subsection{Parameter estimation}

Equation \eqref{eqn:bayesrule} provides the foundation for Bayesian parameter estimation. In this equation $\theta$ represents the unknown parameter vector and $y$ is the data. The probability mass or density functions are the statistical model $p(y|\theta)$ (often referred to as the likelihood), the prior [distribution] $p(\theta)$, the posterior [distribution] $p(\theta|y)$, and the prior predictive [distribution] $p(y)$. 

\begin{example}
\label{exa:coinflip}
Suppose you are trying to determine the probability a coin flip will be heads. Let $Y_i=1$ if the $i$th flip was heads and zero otherwise. Assume the independent and identically distributed Bernoulli statistical model $Y_i\stackrel{iid}{\sim} \Ber(\theta)$ which defines 
\begin{equation}
p(y|\theta) = p(y_1,\ldots,y_n|\theta) = \prod_{i=1}^n \theta^{y_i} (1-\theta)^{1-y_i} = \theta^h (1-\theta)^t
\end{equation}
where $h$ is the number of heads, $t=n-h$ is the number of tails, and $y=(y_1,\ldots,y_n)$. 

If we assume $\theta\sim \Unif(0,1)$ which defines the prior $p(\theta)=\I(0<\theta<1)$, then we have $p(\theta|y) = \Beta(h+1,t+1)$. 

\end{example}

The posterior is the goal of a Bayesian analysis. From here point and interval estimates can be obtained. 


\subsection{Model selection}

As typically presented, model selection (or hypothesis testing) usually has a null hypothesis ($H_0$) and an alternative hypothesis ($H_A)$ and the goal is to determine which (of these two) hypotheses are correct. From a Bayesian perspective, we determine the posterior probability of a hypothesis via 

\begin{equation}
p(H_0|y) %= \frac{p(y|H_0)p(H_0)}{p(y)} 
= \frac{p(y|H_0)p(H_0)}{p(y|H_0)p(H_0)+ p(y|H_A)p(H_A)} = \frac{1}{1+\frac{p(y|H_A)}{p(y|H_0)} \frac{p(H_A)}{p(H_0)}} \label{eqn:modelprob}
\end{equation} 

where $p(y|H_A)/p(y|H_0)$ is the \emph{Bayes' factor} for comparing the alternative hypothesis to the null hypothesis, $p(H_0)$ is the prior probability that model $H_0$ is true, and $p(H_0|y)$ is the posterior probability that model $H_0$ is true. 

\begin{example}
Suppose your friend Jack flips a coin once and it turns up heads and then asks you what kind of coin he has. Since Jack likes to play practical jokes, you suspect that Jack either 1) is using fair coin or 2) is using a coin with two heads. These define two possible models: $H_0: \theta=0.5$ or $H_A: \theta=1$. Using equation \eqref{eqn:modelprob}, the posterior model probability is $p(H_0|y) = 1/3$. 
\end{example}


\subsection{Prediction}

Statistical models are often built to provide predictions for future events. Suppose the observed data $y_1,\ldots,y_n$ and the future data $\ypred$ are conditionally independent given a model and the parameters in the model, i.e. $y_1,\ldots,y_n,\ypred \stackrel{iid}{\sim} p(y|\theta)$. Then a prediction is produced via 
\begin{equation}
p(\ypred|y) = \int p(\ypred|\theta) p(\theta|y) d\theta 
\end{equation}
where $y=(y_1,\ldots,y_n)$ and $p(\theta|y)$ is obtained via equation \eqref{eqn:bayesrule}. 

\begin{example}
Suppose you have observed 2 heads and 0 tails for a possibly biased coin, what is the prediction for the next coin flip. Let $\theta\sim \Unif(0,1)$ and $y_1,\ldots,y_n,\ypred \stackrel{iid}{\sim} \Ber(\theta)$ where $y_i=1$ represents heads for the $i$th coin flip. From example \ref{exa:coinflip}, we have $\theta|y \sim \Beta(3,1)$. The predictive distribution is 
\begin{equation}
p(\ypred|y) = \frac{\mbox{Beta}(\ypred+3,1-\ypred+1)}{\mbox{Beta}(3,1)}
\end{equation}
where $\mbox{Beta}(a,b) = \Gamma(a)\Gamma(b)/\Gamma(a+b)$ is the Beta function. To determine the predictive probability of the next flip being a heads, calculate $P(\ypred=1|y) = p(1|y) = 0.75$. The predictive distribution says there is 0.75 probability that the next flip is a heads. 
\end{example}


\subsection{Bayesian statistics}

From the discussion here, we can see that Bayesian statistics often uses Bayes' rule for its analysis, but what really defines Bayesian statistics is the use of conditional probabilities, e.g. $p(\theta|y)$, $p(H_0|y)$ and $p(\ypred|y)$. In every case, the conditional probability is of the form $p(K|U)$ where $K$ is everything that is known and $U$ is everything that is unknown. The unknowns are model parameters, models themselves, and predictions. The knowns are the observed data as well as a model or model class. The notation could be expanded to include all the knowns, but this typically makes the notation overly cumbersome. 


\subsection{Why Bayesian?}

Compared to the predominent statistical philosophy based on pvalues and confidence intervals, the Bayesian perspective has some advantages and some disadvantages. 
The advantages are that the Bayesian philosophy is coherent, interpretable, incorporates learning, no data asymptotics, and adheres (better) to the likelihood principle. 
The disadvantages are computation, not the standard, 
In the remainder of this section, we take a brief look at each of these advantages and disadvantages. 

\subsubsection{Advantages}


\paragraph{Coherence} 
Bayesian statistics is coherent in that your goal is always the probability [distribution] of the unknowns conditional on the knowns, i.e. $p(K|U)$. 
As statisiticians, we are accustomed to putting probability distributions on quantities that are unknown, e.g. data before an experiment has been run. 
A Bayesian is just being consistent by assuming a probability distribution on parameters which are also unknown. 

\paragraph{Interpretation} 
Bayesian results including parameter estimation, model probabilities, and predictions are all directly interpretable through probabilities. 
For example, in constructing a 95\% interval from a parameter posterior the interpretation is that there is 95\% probability that the parameter is in that interval. 
A model probability is the probability that model is true.
A 95\% prediction interval states that there is 95\% probability the prediction while lie in that interval. 
These are all inherently interpretable quantities.

\paragraph{No data asymptotics}

Much of the development of confidence intervals are based on asymptotic results that require sample sizes to be large. 

\begin{example}
Consider a binomial experiment with probability of success $\theta$ where $y$ heads are observed out of $n$ trials. Using standard methods, a 95\% confidence interval is constructed based on asymptotic normality via $\hat{\theta} \pm 1.96 \sqrt{\hat{\theta}(1-\hat{\theta})/n}$ where $\hat{\theta}=y/n$. A clear issue with this interval is when $y$ is extremely small or large. 
In particular, if $y$ is 0 or n, then the standard error is zero and the 95\% credible interval is a single point at 0 or 1, respectively. 
\end{example}

In simple models, data asymptotics can be evaluated by determining whether the likelihood is approximately quadratic, i.e. approximately normal, but in more complex models this is hard or impossible.

\paragraph{Statistical learning}

Bayesian statistics offers a natural way of learning as data are collected. 
Suppose we have a data sequence $y_1,y_2,\ldots$ with $y_i\iid p(y|\theta)$ and we are interested in updating our information after each observation. 
If we also have a prior, then we have the following sequence of posteriors
\begin{equation}
p(\theta|y_1) = \frac{p(y_1|\theta)p(\theta)}{p(y_1)}  \qquad p(\theta|y_1,\ldots,y_{i+1}) = \frac{p(y_{i+1}|\theta)p(\theta|y_1,\ldots,y_i)}{p(y_{i+1}|y_1,\ldots,y_i)}
\end{equation}

\paragraph{Likelihood principle}

\begin{example}
Consider two coin-flipping experiments. In the first experiment, the scientist flips the coin 20 times and records 7 heads. In the second experiment, the scientist flips until reaching 7 heads and records a total of 20 coin flips. Assuming iid observations, the experiments are modeled with a binomial and negative binomial, respectively. 
The likelihoods for the two experiments are 
\begin{equation}
\mbox{Bin: } L(\theta|X=3) = {12 \choose 3} \theta^3 (1-\theta)^9 \qquad \mbox{NegBin: } {11\choose 2} \theta^3 (1-\theta)^3.
\end{equation}
Since these likelihoods are proportional to each other, the likelihood principle states that an inference should be the same.
If we are interested in the hypotheses $H_0: \theta=0.5$ vs $H_1: \theta<0.5$, then the pvalue for the experiments are 0.07 and 0.03 for the binomial and negative binomial experiments respectively. 
And if we stick with the standard 0.05 cutoff, we would make different decisions about $\theta$ even though our data are exactly the same, i.e. 3 heads and 9 tails.  
\end{example}


\subsubsection{Disadvantages}

\paragraph{Computation}

We will see that Bayesian analyses typically require much more computation time than the associated non-Bayesian analyses. 
In addition, methods such as Markov chain Monte Carlo or importance sampling are based on asymptotic arguments as the number of samples tends to infinity.
Even worse we are rarely sure whether these asymptotics are met. 
Fortunately, we can increase the samples at the, relatively cheap, cost of computation time. 

\paragraph{Not the standard}

For historical reasons primarily relating to computation (in my opinion), frequentist approaches via pvalues and confidence intervals have become the predominant statistical tools in the 1900s. 
Therefore, in promoting a Bayesian approach to statistics there is considerable momentum to overcome. 

\subsection{Misconceptions}

It is commonly stated that ``Bayesian treat parameters as random while non-Bayesians treat parameters as fixed.'' While this is true, most Bayesian believe that the value of a parameter in a particular model has a fixed, but unknown. Since it is unknown, Bayesians assign it a probability distribution and thereby \emph{treat it as random} just as we treat data prior to its collection as random. 

A better quote may be ``Bayesians treat parameters as unknown while frequentists treat parameters as known.'' 

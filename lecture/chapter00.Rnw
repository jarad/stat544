\chapter{A quick introduction}

Test \cite{gelman2003bayesian}.

\section{Bayes' rule}

Bayes' rule as usually presented is 
\begin{equation}
P(A|B) = \frac{P(B|A)P(A)}{P(B)} 
\end{equation}

where $A$ and $B$ are events.

As an example, consider a pregnant woman has a screening test for Down syndrome and the test is positive. The woman will likely wonder what the probability is that the baby will have Down syndrome. The relevant quantities of interest are the test's sensitivity and specificity which are 0.94 and 0.77 respectively, as well as the prevalence of Down syndrome in the U.S. which is 0.001. The prevalence is the probability the baby will have Down syndrome before we knew the results of the test. 

\begin{example}

Let $D$ indicate a child with Down syndrome and $D^c$ the opposite. Let `+' indicate a positive test result and `-' a negative result. Using this notation, we have $P(+|D) = 0.94$, $P(-|D^c) = 0.77$,  and $P(D) = 0.001.$ 

Using Bayes' rule and quantities, we can update the probability to 

\begin{align*}
P(D|+) &= \frac{P(+|D)P(D)}{P(+)} \\
&= \frac{P(+|D)P(D)}{P(+|D)P(D)+P(+|D^c)P(D^c)} \\
&= \frac{0.94\cdot 0.001}{0.94\cdot 0.001+0.23\cdot 0.999} \\
&\approx 1/250 
\end{align*}

So \emph{if the baby had a 1/1000 chance of having Down syndrome without knowledge of the test}, then the baby's probability is now 1/250. The emphasis is added here because the probability after the test depends on the probability before the test.\footnote{The probability also depends on the sensitivity and specificity of the test, but for now we are assuming these are known.} If, for example, the mother is over 35 (a risk factor for Down syndrome), the prevalence in the general population is not appropriate. Instead, we would need the prevalence in the population for those mothers over 35.                
\end{example}                  

%Similarly, we could calculate the probability of having Down syndrome if the test was negative, i.e. $P(D|-)\approx 1/10,000$.

\section{Bayesian statistics}

Bayes' rule is typically introduced as applying to events, but the mathematics are the same for probability mass and density functions. So, if $\theta$ and $y$ are random variables, then the following Bayes' rule applies

\begin{equation}
p(\theta|y) = \frac{p(y|\theta)p(\theta)}{p(y)} =  \frac{p(y|\theta)p(\theta)}{\int p(y|\theta)p(\theta) d \theta} \label{eqn:bayesrule}
\end{equation}

where the probability mass or density functions are determined by their arguments. The integral is understood to be an integral if $\theta$ is continuous or a summation if $\theta$ is discrete. 

\subsection{Parameter estimation}

Equation \eqref{eqn:bayesrule} provides the foundation for Bayesian parameter estimation. In this equation $\theta$ represents the unknown parameter vector and $y$ is the data. The probability mass or density functions are the statistical model $p(y|\theta)$ (often referred to as the likelihood), the prior [distribution] $p(\theta)$, the posterior [distribution] $p(\theta|y)$, and the prior predictive [distribution] $p(y)$. 

\begin{example}
\label{exa:coinflip}
Suppose you are trying to determine the probability a coin flip will be heads. Let $Y_i=1$ if the $i$th flip was heads and zero otherwise. Assume the independent and identically distributed Bernoulli statistical model $Y_i\stackrel{iid}{\sim} \Ber(\theta)$ which defines 
\begin{equation}
p(y|\theta) = p(y_1,\ldots,y_n|\theta) = \prod_{i=1}^n \theta^{y_i} (1-\theta)^{1-y_i} = \theta^h (1-\theta)^t
\end{equation}
where $h$ is the number of heads, $t=n-h$ is the number of tails, and $y=(y_1,\ldots,y_n)$. 

If we assume $\theta\sim \Unif(0,1)$ which defines the prior $p(\theta)=\I(0<\theta<1)$, then we have $p(\theta|y) = \Beta(h+1,t+1)$. 

\end{example}

The posterior is the goal of a Bayesian analysis. From here point and interval estimates can be obtained. 


\subsection{Model selection}

As typically presented, model selection (or hypothesis testing) usually has a null hypothesis ($H_0$) and an alternative hypothesis ($H_A)$ and the goal is to determine which (of these two) hypotheses are correct. From a Bayesian perspective, we determine the posterior probability of a hypothesis via 

\begin{equation}
p(H_0|y) %= \frac{p(y|H_0)p(H_0)}{p(y)} 
= \frac{p(y|H_0)p(H_0)}{p(y|H_0)p(H_0)+ p(y|H_A)p(H_A)} = \frac{1}{1+\frac{p(y|H_A)}{p(y|H_0)} \frac{p(H_A)}{p(H_0)}} \label{eqn:modelprob}
\end{equation} 

where $p(y|H_A)/p(y|H_0)$ is the \emph{Bayes' factor} for comparing the alternative hypothesis to the null hypothesis, $p(H_0)$ is the prior probability that model $H_0$ is true, and $p(H_0|y)$ is the posterior probability that model $H_0$ is true. 

\begin{example}
Suppose your friend Jack flips a coin once and it turns up heads and then asks you what kind of coin he has. Since Jack likes to play practical jokes, you suspect that Jack either 1) is using fair coin or 2) is using a coin with two heads. These define two possible models: $H_0: \theta=0.5$ or $H_A: \theta=1$. Using equation \eqref{eqn:modelprob}, the posterior model probability is $p(H_0|y) = 1/3$. 
\end{example}


\subsection{Prediction}

Statistical models are often built to provide predictions for future events. Suppose the observed data $y_1,\ldots,y_n$ and the future data $\ypred$ are conditionally independent given a model and the parameters in the model, i.e. $y_1,\ldots,y_n,\ypred \stackrel{iid}{\sim} p(y|\theta)$. Then a prediction is produced via 
\begin{equation}
p(\ypred|y) = \int p(\ypred|\theta) p(\theta|y) d\theta 
\end{equation}
where $y=(y_1,\ldots,y_n)$ and $p(\theta|y)$ is obtained via equation \eqref{eqn:bayesrule}. 

\begin{example}
Suppose you have observed 2 heads and 0 tails for a possibly biased coin, what is the prediction for the next coin flip. Let $\theta\sim \Unif(0,1)$ and $y_1,\ldots,y_n,\ypred \stackrel{iid}{\sim} \Ber(\theta)$ where $y_i=1$ represents heads for the $i$th coin flip. From example \ref{exa:coinflip}, we have $\theta|y \sim \Beta(3,1)$. The predictive distribution is 
\begin{equation}
p(\ypred|y) = \frac{\mbox{Beta}(\ypred+3,1-ypred+1}{\mbox{Beta}(3,1)}
\end{equation}
where $\mbox{Beta}(a,b) = \Gamma(a)\Gamma(b)/\Gamma(a+b)$ is the Beta function. Since $\ypred$ is either 0 or 1, we can calculate $P(\ypred=1|y) = p(1|y) = 0.75$, so the predictive distribution says there is 0.75 probability that the next flip is a heads. 

\subsection{Bayesian statistics}

From the discussion here, we can see that Bayesian statistics often uses Bayes' rule for its analysis, but what really defines Bayesian statistics is the use of conditional probabilities, e.g. $p(\theta|y)$, $p(H_0|y)$ and $p(\ypred|y)$. In every case, the conditional probability is of the form $p(K|U)$ where $K$ is everything that is known and $U$ is everything that is unknown. The notation has been simplified for parsimonious presentation by eliminating anything that is considered known for the entire analysis, e.g. the model. We could make the notation more exact via, for example, $p(\theta|y,M)$ where $M$ represents the assumed model, e.g. Bernoulli. 


\end{example}


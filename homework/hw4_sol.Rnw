\documentclass[12pt]{article}
\usepackage{amsmath,amssymb,mathrsfs,fancyhdr,syntonly,lastpage,hyperref,enumitem,graphicx,verbatim}

\hypersetup{colorlinks=true,urlcolor=black}

\topmargin      -1.5cm   % read Lamport p.163
\oddsidemargin  -0.04cm  % read Lamport p.163
\evensidemargin -0.04cm  % same as oddsidemargin but for left-hand pages
\textwidth      16.59cm
\textheight     23.94cm
\parskip         7.2pt   % sets spacing between paragraphs
\parindent         0pt   % sets leading space for paragraphs
\pagestyle{empty}        % Uncomment if don't want page numbers
\pagestyle{fancyplain}



\begin{document}
\lhead{Homework 4}
\chead{PSTAT 544 - Bayesian statistics}
\rhead{Page \thepage\ of \pageref{LastPage}}
 
<<echo=FALSE>>=
library(ggplot2)
library(plyr)
@

\begin{enumerate}
\item Consider the model $Y_i\stackrel{iid}{\sim} N(\mu,\sigma^2)$ with $\mu$ and $\sigma^2$ both unknown.
  \begin{enumerate}
  \item Derive the posterior under the fully conjugate prior 
  \[ p(\mu,\sigma^2) = p(\mu|\sigma^2)p(\mu) = N(\mu|m,\sigma^2/k)\mbox{Inv-}\chi^2(\sigma^2|v,s^2). \]
  
\paragraph{Solution}

The prior has the form 
\[ p(\mu,\sigma^2) \propto  \sigma^{-1}(\sigma^2)^{-(v/2+1)}\exp\left(-\frac{1}{2\sigma^2} [vs^2 + k(m-\mu)^2]  \right) \]
from equation 3.6 in BDA3 and the likelihood can be written as 
\[ L(\mu,\sigma^2) = p(y|\mu,\sigma^2) \propto (\sigma^2)^{-n/2} \exp\left( -\frac{1}{2\sigma^2}[(n-1)S^2+n(\overline{y}-\mu)^2]\right)  \]
from equation 3.7 in BDA3. The posterior is 
\begin{align*}
p(\mu,\sigma^2|y) \propto& p(y|\mu,\sigma^2)p(\mu,\sigma^2) \\
=& \sigma^{-1}(\sigma^2)^{-(v/2+1)}\exp\left(-\frac{1}{2\sigma^2} [vs^2 + k(m-\mu)^2]  \right) \\
&\times (\sigma^2)^{-n/2} \exp\left( -\frac{1}{2\sigma^2}[(n-1)S^2+n(\overline{y}-\mu)^2]\right) \\
&= \sigma^{-1}(\sigma^2)^{-([v+n]/2+1)} \exp\left(-\frac{1}{2\sigma^2} [vs^2+ (n-1)S^2 + k(m-\mu)^2 +n(\overline{y}-\mu)^2]\right)
\end{align*}

Now deal with the $k(m-\mu)^2 +n(\overline{y}-\mu)^2$ term in the exponent
{\footnotesize
\begin{align*}
k(m-\mu)^2 +n(\overline{y}-\mu)^2 &= [k+n]\mu^2 - 2\mu(km+n\overline{y}) + km^2 +n\overline{y} \\
&= km^2 +n\overline{y} +[k+n]\left( \mu^2 - 2\mu\frac{km+n\overline{y}}{k+n}\right) \\
&= km^2 +n\overline{y} +[k+n]\left( \mu^2 - 2\mu\frac{km+n\overline{y}}{k+n} + \left[ \frac{km+n\overline{y}}{k+n} \right]^2 - \left[ \frac{km+n\overline{y}}{k+n} \right]^2  \right) \\
&= km^2 +n\overline{y} -\frac{k^2m^2+2kmn\overline{y}+n^2\overline{y}^2}{k+n}+[k+n]\left( \mu-\frac{km+n\overline{y}}{k+n} \right)^2 \\
&= \frac{kn}{k+n}(\overline{y}-m)^2+[k+n]\left( \mu-\frac{km+n\overline{y}}{k+n} \right)^2 
\end{align*}
}  

Plugging this back in, we have 
{\footnotesize
\[ p(\mu,\sigma^2|y) \propto \sigma^{-1}(\sigma^2)^{-([v+n]/2+1)} \exp\left(-\frac{1}{2\sigma^2} \left[vs^2+ (n-1)S^2 + \frac{kn}{k+n}(\overline{y}-m)^2+[k+n]\left( \mu-\frac{km+n\overline{y}}{k+n} \right)^2 \right] \right). \]
}
This matches the form of the prior, so the posterior must be $N(\mu|m',\sigma^2/k')\mbox{Inv-}\chi^2(\sigma^2|v',(s')^2)$ with

\begin{align*}
k' &= k+n \\
m' &= \frac{km+n\overline{y}}{k+n} \\
v' &= v+n \\
v'(s')^2 &= vs^2+ (n-1)S^2 + \frac{kn}{k+n}(\overline{y}-m)^2
\end{align*}
  
  
  \item Derive the posterior under the noninformative prior $p(\mu,\sigma^2)\propto 1/\sigma^2$. 
  
  \paragraph{Solution}
  
The posterior is 
\begin{align*}
p(\mu,\sigma^2|y) &\propto (\sigma^2)^{-1} (\sigma^2)^{-n/2} \exp\left( -\frac{1}{2\sigma^2}[(n-1)S^2+n(\overline{y}-\mu)^2]\right) \\
&= \sigma^{-1} (\sigma^2)^{-[n-1]/2} \exp\left( -\frac{1}{2\sigma^2}[(n-1)S^2+n(\overline{y}-\mu)^2]\right)
\end{align*}
which is $N(\mu|m',\sigma^2/k')\mbox{Inv-}\chi^2(\sigma^2|v',(s')^2)$ with 

\begin{align*}
k' &= n \\
m' &= \overline{y} \\
v' &= n-1 \\
v'(s')^2 &= (n-1)S^2 
\end{align*}  
  
  \item The prior in b) is the limit of the prior in a) as $k,v\to 0$. Show that the limit of the posterior in b) is not the limit of the posterior in a) as $k,v\to 0$. [Note: this is trivial but I want to make sure you understand the point, i.e. you cannot simpy plug $k=v=0$ into the posterior from a) to get the posterior from b).]
  
\paragraph{Solution}

Using the non-informative prior, we have $v'=n-1$ which is not equal to limit of the posterior in part a $\lim_{k,v\to 0} n+v = n$. 
  
  \end{enumerate}

\item Consider the model $Y_i \stackrel{iid}{\sim} Po(\lambda)$ with observed data $y=(4,4,5,8,3)$. Perform the following ``hypothesis tests'' making sure to identify your assumptions. Provide posterior probabilities for each hypothesis. 
  \begin{enumerate}
  \item $H_0:\lambda= 5$ vs $H_1:\lambda\ne 5$
  
\paragraph{Solution}  
  
Since this is a point null hypothesis, we need to a formal Bayesian hypothesis test. Assume $p(H_0)=1-p(H_1)=0.5$ with $\lambda|H_1 \sim Ga(a,b)$, then the prior predictive distribution under $H_0$ is $Po(5)$ and the prior predictive distribution under $H_1$ is 
\begin{align*} 
p(y|H_1) &= \int p(y|\lambda)p(\lambda|H_1) d\lambda \\
&= \left. \frac{b^a}{\Gamma(a)} \int \lambda^{n\overline{y}}  e^{-n\lambda} \lambda^{a-1} e^{-b\lambda} d\lambda \right/ \prod_{i=1}^n y_i! \\
&= \left. \frac{b^a}{\Gamma(a)} \int \lambda^{a+n\overline{y}-1} e^{-[b+n]\lambda} d\lambda \right/ \prod_{i=1}^n y_i! \\
&= \left. \frac{b^a}{\Gamma(a)} \frac{\Gamma(a+n\overline{y})}{[b+n]^{a+n\overline{y}}} \right/ \prod_{i=1}^n y_i! 
\end{align*}

The Bayes factor for $H_0$ compared to $H_1$ is 
\[ 
BF(H_0:H_1) = \frac{p(y|H_0)}{p(y|H_1)} 
= \frac{ \left. 5^{n\overline{y}}  e^{-5n} \right/ \prod_{i=1}^n y_i!}{\left. \frac{b^a}{\Gamma(a)} \frac{\Gamma(a+n\overline{y})}{[b+n]^[a+n\overline{y}]} \right/ \prod_{i=1}^n y_i!} 
=\frac{5^{n\overline{y}}  e^{-5n} \Gamma{a} [b+n]^{a+n\overline{y}}}{b^a \Gamma(a+n\overline{y})}
\]

<<bayes_factor>>=
bf = function(y,a=1,b=1) {
  n = length(y)
  sumy = sum(y)
  lbf = sum(y)*log(5) -5*n + lgamma(a) + (a+sum(y))*log(b+n) - a*log(b) - lgamma(a+sum(y))
  exp(lbf)
}
@

The posterior probability of the null hypothesis when $\lambda|H_1 \sim Ga(1,1)$. 

<<posterior_2a>>=
y = c(4,4,5,8,3)
1/(1+1/bf(y))
@

This is not surprising since the data are consistent with $\lambda=5$ and our prior is relatively vague. We could consider other proper priors to determine the effect on the Bayes factor and therefore the posterior probability. 

<<effect_of_prior, fig.width=4, fig.height=4>>=
df_bf = function(x) data.frame(bf=bf(y,x$a,x$b))
d = ddply(expand.grid(a=5*(1:9),b=1:9), .(a,b), df_bf)
p = ggplot(d, aes(a,b,fill=log(1/bf))) + geom_tile()
print(p+labs(title="BF(H_1:H_0)"))
@

From this figure, we can see that the Bayes Factor for the alternative to the null is maximized when $a/b=\overline{y}\approx 5$ and, for $a/b=\overline{y}$, increases as $a$ and $b$ increase.


  \item $H_0:\lambda<5$ vs $H_1:\lambda \ge 5$
  
\paragraph{Solution}  

Rather than treating this problem as a formal Bayesian hypothesis test, it is easier and more reasonable to treat this as a parameter estimation problem and then calculate the probabilities $p(\lambda<5|y)$ and $p(\lambda\ge 5|y)$. Since we are not doing a formal Bayesian test, we can use Jeffreys prior and thus the posterior is $\lambda|y\sim Ga(n\overline{y}+.5,n)$. So the probability that $\lambda$ is less than 5 is

<<less_than_5>>=
pgamma(5,sum(y)+.5,length(y)) # p(H_0|y)=1-p(H_1|y)
@
  
  \item $H_i:\lambda=\lambda_i=i$ for $i=0,\ldots,10$. 
  
\paragraph{Solution}

This problem could be treated as either a formal Bayesian hypothesis test or as a parameter estimation problem where $p(\lambda=i)=p(H_i)=p_i$ and will result in the same probabilities. Using the discrete prior, the posterior probability is 
\[ p(\lambda=i|y) = \frac{p(y|\lambda=i)p(\lambda=i)}{\sum_{j=0}^{10} p(y|\lambda=j)p(\lambda=j)} \propto \prod_{k=1}^n Po(y_k;i) \]

<<discrete_prior>>=
d = ddply(data.frame(lambda = 0:10, p=1/11), .(lambda,p), summarize, 
          like = exp(sum(dpois(y,lambda,log=TRUE))), post = p*like)
d$post = d$post / sum(d$post)
round(d[,c("lambda","post")],2)
@
  
  \item $H_i:\lambda\in(\lambda_{i-1},\lambda_i]$ for $i=1,\ldots,10$, $\lambda_i=i$ for $i<10$, and $\lambda_{10}=\infty$. 
  
\paragraph{Solution}

This should also be approached as a parameter estimation problem and finding the posterior probability of $\lambda$ falling in each interval.

<<intervals>>=
endpoints = c(0:9,Inf)
ints = data.frame(lower=endpoints[-length(endpoints)],
                  upper=endpoints[-1])
ints$prob = pgamma(ints$upper, sum(y)+.5, length(y)) - pgamma(ints$lower, sum(y)+.5, length(y)) 
round(ints,2)
@

  \end{enumerate}


\end{enumerate}




\end{document}

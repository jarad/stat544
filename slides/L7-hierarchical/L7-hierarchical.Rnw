\documentclass[handout,xcolor=pdftex,dvipsnames,table]{beamer} % for handouts
%\documentclass{beamer}

\usecolortheme[RGB={0,0,144}]{structure}
\usetheme{AnnArbor}\usecolortheme{beaver}
%\usetheme{CambridgeUS}\usecolortheme{crane}

\usepackage{verbatim,xmpmulti,color,multicol,multirow}
\setlength{\unitlength}{\textwidth}  % measure in textwidths
\usepackage[normalem]{ulem}

%\usepackage{beamerthemesplit}
\setbeamertemplate{navigation symbols}{}
%\setbeamercolor{alerted text}{fg=red}
%\setbeamertemplate{block body theorem}{bg=orange}
\setkeys{Gin}{width=0.6\textwidth}

\title{Hierarchical models}
\author[Jarad Niemi]{Dr. Jarad Niemi}
\institute[Iowa State]{Iowa State University}
\date{\today}

\begin{document}

%\section{Temp??} \begin{comment}

<<chunk_options, echo=FALSE, message=FALSE>>=
opts_chunk$set(fig.width=6, fig.height=5, out.width='.8\\linewidth', fig.align='center', size='tiny')
####################################
# L7 - Hierarchical models         #
####################################
library(reshape2)
library(plyr)
library(ggplot2)
library(rjags)
@

\frame{\maketitle}

\section{Modeling}
\frame{\frametitle{Airline accident rates}
  Suppose $Y_i$ are the number of fatal accidents in year $i$, \pause and assume
  \[ Y_i \stackrel{ind}{\sim} Po(\lambda_i x_i) \]
  where 
  \begin{itemize}[<+->]
  \item $x_i$ is the number 100 million miles flown in year $i$ and
  \item $\lambda_i$ is the accident rate in year $i$.
  \end{itemize}
  
  \vspace{0.2in} \pause
  
  Do these models make sense?
  \begin{itemize}[<+->]
  \item The accident rate every year is the same, i.e. $\lambda_i=\lambda$. 
  \item The accident rate every year is independent of other years.
  \item The accident rate each year should be similar to other years. 
  \end{itemize}
}


\frame{\frametitle{Andre Dawkin's three-point percentage}
  Suppose $Y_i$ are the number 3-pointers Andre Dawkin's makes in season $i$, \pause and assume
  \[ Y_i \stackrel{ind}{\sim} Bin(n_i,\theta_i) \]
  where 
  \begin{itemize}[<+->]
  \item $n_i$ are the number of 3-pointers attempted and
  \item $\theta_i$ is the probability of making a 3-pointer in season$i$.
  \end{itemize}
  
  \vspace{0.2in} \pause
  
  Do these models make sense?
  \begin{itemize}[<+->]
  \item The 3-point percentage every season is the same, i.e. $\theta_i=\theta$. 
  \item The 3-point percentage every season is independent of other seasons. 
  \item The 3-point percentage every season should be similar to other seasons. 
  \end{itemize}
}


\frame{\frametitle{Andre Dawkin's three-point percentage}
  Suppose $Y_i$ are the number 3-pointers Andre Dawkin's makes in \alert{game} $i$, \pause and assume
  \[ Y_i \stackrel{ind}{\sim} Bin(n_i,\theta_i) \]
  where 
  \begin{itemize}[<+->]
  \item $n_i$ are the number of 3-pointers attempted in game $i$ and
  \item $\theta_i$ is the probability of making a 3-pointer in game $i$.
  \end{itemize}
  
  \vspace{0.2in} \pause
  
  Do these models make sense?
  \begin{itemize}[<+->]
  \item The 3-point percentage every game is the same, i.e. $\theta_i=\theta$. 
  \item The 3-point percentage every game is independent of other games. 
  \item The 3-point percentage every game should be similar to other games. 
  \end{itemize}
}


\begin{frame}[fragile]
\frametitle{Andre Dawkin's 3-point percentage}
<<dawkins_data, eval=FALSE>>=
d = read.csv("dawkins.csv")
d = rbind(d, data.frame(date=NA, opponent='Total', made=sum(d$made), attempts=sum(d$attempts)))
d$a = 0.5 + d$made
d$b = 0.5 + d$attempts-d$made
d$lcl = qbeta(.025, d$a, d$b)
d$ucl = qbeta(.975, d$a, d$b)
d$game = 1:nrow(d)
p = ggplot(d, aes(x=lcl, xend=ucl, y=game, yend=game,color=1+(opponent=="Total")))+
  geom_segment(lwd=2)+ theme(legend.position="none")+labs(x=expression(theta))
print(p)
d = d[d$opponent!="Total",]
@
\end{frame}

\begin{frame}[fragile]
\frametitle{Andre Dawkin's 3-point percentage}
<<dawkins_data_plot, echo=FALSE>>=
<<dawkins_data>>
@
\end{frame}


\section{Hierarchical models}
\frame{\frametitle{Hierarchical models}
  Consider the following model
	\[ \begin{array}{ll}
	y_i &\stackrel{ind}{\sim} p(y|\theta_i) \pause \\
	\theta_i &\stackrel{iid}{\sim} p(\theta|\phi) \pause \\
	\phi &\sim p(\phi)
	\end{array} \]
  \pause where 
  \begin{itemize}
  \item $y_i$ is observed\pause, 
	\item $\theta=(\theta_1,\ldots,\theta_n)$ and $\phi$ are parameters\pause, and 
	\item only $\phi$ has a prior that is set.
	\end{itemize}
	
	\vspace{0.2in} \pause 
	
	This is a hierarchical or multilevel model.
}

\subsection{Posteriors}
\frame{\frametitle{Posterior distribution for hierarchical models}
	The joint posterior distribution of interest in hierarchical models is 
	\[ p(\theta,\phi|y) \pause \propto p(y|\theta,\phi)p(\theta,\phi) \pause = p(y|\theta)p(\theta|\phi)p(\phi). \]
	\pause We may also be focused on marginal posteriors
	\[ p(\theta|y) \pause = \int p(\theta,\phi|y) d\phi \qquad \pause \mbox{ or } \qquad p(\phi|y) \pause = \int p(\theta,\phi|y) d\theta .\]
	
	\vspace{0.2in} \pause
	
	Comments:
	\begin{itemize}
	\item $p(\mbox{unknown}|\mbox{known})$ are assumptions you make\pause, and
	\item pay close attention to conditional independence.
	\end{itemize}
}

\subsection{Example}
\frame{\frametitle{Three-pointer example}
	Our statistical model 
	\[ \begin{array}{ll}
	Y_i &\stackrel{ind}{\sim} Bin(n_i,\theta_i) \pause \\
	\theta_i&\stackrel{iid}{\sim} Be(\alpha,\beta) \pause \\
	\alpha,\beta &\sim p(\alpha,\beta) 
	\end{array} \]
	
	\vspace{0.2in} \pause
	
	In this example,
	\begin{itemize}
	\item $\phi=(\alpha,\beta)$\pause\,  and
	\item $\alpha,\beta$ describes the variability in 3-point percentage across games\pause, and
	\item we are going to learn about this variability. 
	\end{itemize}
}

\subsection{Prior}
\frame{\frametitle{A prior distribution for $\alpha$ and $\beta$}
  Recall the interpretation: \pause
  \begin{itemize}[<+->]
  \item $\alpha$: prior successes
  \item $\beta$: prior failures
  \end{itemize}
  
  \vspace{0.2in} \pause 
  
  A more natural parameterization is 
  \begin{itemize}[<+->]
  \item prior expectation: $\mu = \frac{\alpha}{\alpha+\beta}$
  \item prior sample size: $\eta = \alpha + \beta$
  \end{itemize}
  
  \vspace{0.2in} \pause 
  
  Place priors on these parameters or transformed to the real line:
  \begin{itemize}[<+->]
  \item $\mbox{logit } \mu = \log(\mu/[1-\mu]) = \log(\alpha/\beta)$
  \item $\log \eta$
  \end{itemize}
}

\frame{\frametitle{A prior distribution for $\alpha$ and $\beta$}
  It seems reasonable to assume the mean ($\mu$) and size ($\eta$) are independent \emph{a priori}:
  \[ p(\mu,\eta) = p(\mu)p(\eta) \]
  
  \vspace{0.2in} \pause
  
  Let's assume a vague proper prior for $\mu$ and $\eta$ \pause perhaps
  \begin{itemize}
  \item $\mu \sim Be(1,1)$
  \item $\eta \sim LN(0,1)$
  \end{itemize}
  \pause where $LN(0,1)$ is a log-normal distribution, i.e. $\log(\eta) \sim N(0,1)$.
}

\begin{frame}[fragile]
\frametitle{Prior draws}
<<proper_prior>>=
n = 1e4
end = 100
size = rlnorm(n, 0, 1)
mean = runif(n, 0 , 1)
summary(size)
summary(mean)
alpha = size*mean
beta  = size*(1-mean)
summary(alpha)
summary(beta)
@
\end{frame}



\begin{frame}[fragile]
<<proper_prior_plot, fig.width=8>>=
par(mfrow=c(2,3))
hist(size)
hist(mean)
plot(size, mean, log='x')
hist(alpha)
hist(beta)
plot(alpha,beta)
@
\end{frame}



\section{JAGS}
\begin{frame}[fragile]
\frametitle{JAGS}
<<jags, eval=FALSE>>=
model = "
model {
  for (i in 1:length(y)) {
    y[i] ~ dbin(theta[i], n[i])
    theta[i] ~ dbeta(alpha, beta)
  }
  alpha <- size*mean
  beta  <- size*(1-mean)

  mean ~ dbeta(1,1)
  size ~ dlnorm(0,1)
}
"

dat = list(y=d$made, n=d$attempts)

m = jags.model(textConnection(model), data=dat)
res = coda.samples(m, c("theta","mean","size"), 10000)
plot(res[,c("mean","size")])
@
\end{frame}


\begin{frame}[fragile]
\frametitle{JAGS}
<<jags_run, echo=FALSE, cache=TRUE, message=FALSE, fig.width=8>>=
<<jags>>
@
\end{frame}


\begin{frame}[fragile]
\frametitle{Comparing independent and hierarchical models}
<<quantiles, dependson="jags_run", eval=FALSE>>=
tmp = data.frame(summary(res)$quantiles)
d$model = "independent"
new_d = d
new_d$model = "hierarchical"
new_d$lcl = tmp[-c(1:2),1]
new_d$ucl = tmp[-c(1:2),5]
combined = rbind(d, new_d)

e = 0.2
p = ggplot(combined, aes(x=lcl, xend=ucl, y=game+e*(model=="hierarchical"), yend=game+e*(model=="hierarchical"), color=model))+
  geom_segment(lwd=2, alpha=0.5) + labs(x=expression(theta), y="game")
print(p)
@
\end{frame}

\begin{frame}[fragile]
\frametitle{}
<<quantiles_plot, echo=FALSE>>=
<<quantiles>>
@
\end{frame}





\frame{\frametitle{A prior distribution for $\alpha$ and $\beta$}
  In Bayesian Data Analysis (3rd ed) page 110, several priors are discussed 
  
  \vspace{0.2in} \pause
  
	\begin{itemize}
	\item $(\log(\alpha/\beta), \log(\alpha+\beta)) \propto 1$ leads to an improper posterior.
  
  \vspace{0.2in} \pause
  
  \item $(\log(\alpha/\beta), \log(\alpha+\beta)) \sim Unif([-10^{10},10^{10}] \times [-10^{10},10^{10}])$ \pause while proper and seemingly vague is a very informative prior.
  
  \vspace{0.2in} \pause
	
  \item $(\log(\alpha/\beta), \log(\alpha+\beta)) \propto \alpha\beta(\alpha+\beta)^{-5/2}$ \pause which leads to a proper posterior \pause and is equivalent to $p(\alpha,\beta) \propto (\alpha+\beta)^{-5/2}$.
	\end{itemize}
}

\begin{frame}[fragile]
<<vague_prior, fig.width=8>>=
end = 2
size = exp(runif(n, -10^end, 10^end))
mean = 1/(1+1/exp(runif(n, -10^end, 10^end)))
plot(size,mean,log='x', main="Seemingly vague prior")
@
\end{frame}

\frame{\frametitle{Zeros trick in JAGS/BUGS}
  Recall the Poisson pmf:
  \[ p(z|\lambda) = \frac{\lambda^z e^{-\lambda}}{z!} \]
  so if $z=0$, this contributes $e^{-\lambda}$ to the likelihood. 
  
  \vspace{0.2in} \pause 
  
  So, we can incorporate any prior (or likelihood) we want in JAGS \pause by introducing a fictitious data point $z=0$ with $\lambda$ equal to the $-\log$ of the prior\pause, \pause e.g. set 
  \[ \lambda = 5/2 \log(\alpha+\beta). \]
  \pause then 
  \[ e^{-\lambda} = (\alpha+\beta)^{-5/2} \]
  will be added to the model. 
  
  \vspace{0.2in} \pause
  
  JAGS does not allow improper priors, so we will have to truncate to some large uniform distribution. 
}


\begin{frame}[fragile]
\frametitle{JAGS}
<<jags2, eval=FALSE>>=
model = "
model {
  for (i in 1:length(y)) {
    y[i] ~ dbin(theta[i], n[i])
    theta[i] ~ dbeta(alpha, beta)
  }
  alpha ~ dunif(0, 1000)
  beta  ~ dunif(0, 1000)

  phi <- 5/2 * log(alpha+beta)
  zero ~ dpois(phi)

  mean <- alpha/(alpha+beta)
  size <- alpha+beta
}
"

dat = list(y=d$made, n=d$attempts, zero=0)

m = jags.model(textConnection(model), data=dat, n.adapt=2e4)
res = coda.samples(m, c("theta","mean","size"), 2e4)
plot(res[,c("mean","size")])
@
\end{frame}



\begin{frame}[fragile]
\frametitle{JAGS}
<<jags2_run, echo=FALSE, cache=TRUE, message=FALSE, fig.width=8>>=
<<jags2>>
@
\end{frame}


\begin{frame}[fragile]
\frametitle{Comparing the independent and hierarchical models}
<<quantiles2, dependson="jags_run", eval=FALSE>>=
tmp = data.frame(summary(res)$quantiles)
d$model = "independent"
new_d = d
new_d$model = "hierarchical"
new_d$lcl = tmp[-c(1:2),1]
new_d$ucl = tmp[-c(1:2),5]
combined = rbind(d, new_d)

e = 0.2
p = ggplot(combined, aes(x=lcl, xend=ucl, y=game+e*(model=="hierarchical"), yend=game+e*(model=="hierarchical"), color=model))+
  geom_segment(lwd=2, alpha=0.5) + labs(x=expression(theta), y="game")
print(p)
@
\end{frame}

\begin{frame}[fragile]
\frametitle{}
<<quantiles2_plot, echo=FALSE>>=
<<quantiles>>
@
\end{frame}



\subsection{Summary}
\frame{\frametitle{Summary}
  Hierarchical mdoels: \pause
	\begin{itemize}[<+->]
	\item Basic structure
	\[ y\sim p(y|\theta) \qquad \theta\sim p(\theta|\phi) \qquad \phi\sim p(\phi) \phantom{|\psi\qquad \psi\sim p(\psi)} \]
	\item \pause Extension (one more level in the hierarchy)
	\[ y\sim p(y|\theta) \qquad \theta\sim p(\theta|\phi) \qquad \phi\sim p(\phi|\psi) \qquad \psi\sim p(\psi) \]
	\item \pause When deriving posteriors, remember the conditional independence structure, \pause e.g.
	\[ p(\theta,\phi,\psi|y) \propto p(y|\theta) p(\theta|\phi) p(\phi|\psi) p(\psi) \]
	\end{itemize}
}



\end{document}

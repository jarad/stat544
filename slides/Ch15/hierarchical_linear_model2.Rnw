\documentclass[handout,xcolor=pdftex,dvipsnames,table]{beamer} % for handouts
%\documentclass{beamer}


\usecolortheme[RGB={0,0,144}]{structure}
\usetheme{AnnArbor}\usecolortheme{beaver}
%\usetheme{CambridgeUS}\usecolortheme{crane}

\usepackage{verbatim,xmpmulti,color,multicol,multirow,tikz}
\setlength{\unitlength}{\textwidth}  % measure in textwidths
\usepackage[normalem]{ulem}

%\usepackage{beamerthemesplit}
\setbeamertemplate{navigation symbols}{}
%\setbeamercolor{alerted text}{fg=red}
%\setbeamertemplate{block body theorem}{bg=orange}
\setkeys{Gin}{width=0.6\textwidth}


\title{Hierarchical linear models}
\author[Jarad Niemi]{Dr. Jarad Niemi}
\institute[Iowa State]{Iowa State University}
\date{\today}

\newcommand{\mG}{\mathrm{\Gamma}}
\newcommand{\I}{\mathrm{I}}
\newcommand{\mySigma}{\mathrm{\Sigma}}


\begin{document}

%\section{Temp??} \begin{comment}

<<chunk_options, echo=FALSE, message=FALSE>>=
require(knitr) # only needed so the following command does not fail when sourcing R code
opts_chunk$set(fig.width=6, fig.height=5, out.width='.8\\linewidth', fig.align='center', size='tiny')
##############################################
# Random intercept, random slope             #
##############################################
require(reshape2)
require(dplyr)
require(plyr)
require(ggplot2)
require(MASS)
#require(rjags)
#require(rstan)
#require(lme4)
set.seed(20140422)
@

\frame{\maketitle}

\begin{frame}
\frametitle{Random intercept, random slope model} 

  Let $y_{ij}$ be the observation for individual $i$ of group $j$ with explanatory variable $x_{ij}$ where $i=1,\ldots,n_j$ and $j=1,\ldots,J$ . \pause Then a \alert{random intercept, random slope model} is 
  \[ \begin{array}{rl}
  y_{ij} &\sim N(\beta_{0,j}+x_{ij}\beta_{1,j} ,\sigma_y^2) \\
  \beta_j = \left( \begin{array}{c} 
  \beta_{0,j} \\
  \beta_{1,j}
  \end{array} \right) &\sim N(\mu_\beta,\mySigma_\beta) 
  \end{array} \]
  
\end{frame}


\begin{frame}[fragile]

<<random_intercepts_and_slopes>>=
J = 20
Sigma_beta = matrix(c(1,.5,.5,1), 2, 2)
beta = mvrnorm(J, rnorm(2), Sigma_beta)
plot(beta)
@

\end{frame}


\begin{frame}[fragile]
<<simulate_data>>=
n = rpois(J, 5) + 1 # Make sure all are greater than zero
group = rep(1:J, n)
table(group)
sigma_y = 1
x = rnorm(sum(n))
y = rnorm(sum(n), beta[group,1]+beta[group,2]*x, sigma_y)
d = data.frame(y=y, group=factor(group), x=x)
@
\end{frame}

\begin{frame}[fragile]
<<plot_data>>=
(p = ggplot(d, aes(x=x,y=y))+geom_point()+stat_smooth(method="lm", se=FALSE))
p + facet_wrap(~group)
@
\end{frame}



\begin{frame}[fragile]
<<>>=
m = lmer(y~(x|group), d)
plot(ranef(m)$group)
points(beta, pch=19)
@
\end{frame}


\section{Bayesian analysis}

\begin{frame}
\frametitle{Random intercept, random slope model} 

  Let $y_{ij}$ be the observation for individual $i$ of group $j$ with explanatory variable $x_{ij}$ where $i=1,\ldots,n_j$ and $j=1,\ldots,J$ . \pause Then a \alert{random intercept, random slope model} is 
  \[ \begin{array}{rl}
  y_{ij} &\sim N(\beta_{0,j}+x_{ij}\beta_{1,j} ,\sigma_y^2) \\
  \beta_j &\sim N(\mu_\beta,\mySigma_\beta) 
  \end{array} \]
  
  \vspace{0.2in} \pause
  
  For a Bayesian analysis, we need to specify a prior on $(\sigma_y^2,\mu_\beta,\mySigma_\beta)$. \pause Typically, 
  \begin{itemize}
  \item $p(\sigma_y^2) \propto 1/\sigma_y^2$ \pause
  \item $p(\mu_\beta) \propto 1$ \pause
  \end{itemize}
  but what should we do for $\mySigma$?
  
\end{frame}




\begin{frame}
\frametitle{Conjugate prior for a covariance matrix}

The natural conjugate prior for a covariance matrix is the \alert{inverse-Wishart} distribution, which has density
\[ p(\mySigma) \propto |\mySigma|^{-(\nu+d+1)/2}\exp\left(-\frac{1}{2} \mbox{tr}\left(S\mySigma^{-1}\right) \right) \]
with $\nu>d-1$ and $S$ is a positive definite matrix where 
\[ E[\mySigma] = \frac{S}{\nu-d-1} \]
for $\nu>d+1$.

\vspace{0.2in} \pause 

Special cases:
\begin{itemize}
\item If $\nu=d+1$, then each of the correlations in $\Sigma$ has a marginal uniform prior. 
\item As $\nu\to-1$ and $|S|\to 0$, we have Jeffreys prior 
\[ p(\mySigma) = |\mySigma|^{-(d+1)/2} \]
\end{itemize}

\end{frame}


\begin{frame}
\frametitle{Full Bayesian model}

Consider the random intercept, random slope model
\[ \begin{array}{rl}
  y_{ij} &\sim N(\beta_{0,j}+x_{ij}\beta_{1,j} ,\sigma_y^2) \\
\beta_j &\sim N(\mu_\beta,\mySigma_\beta) 
\end{array} \]
with independent priors 
\[ \begin{array}{rl}
p(\sigma_y^2) &\propto 1/\sigma_y^2 \\
p(\mu_\beta) &\propto 1 \\
\mySigma_\beta &\sim \mbox{IW}(d+1,\mathrm{I})
\end{array} \]
\end{frame}


\section{Gibbs sampling}
\begin{frame}
\frametitle{Block Gibbs sampler}

Consider the following block Gibbs sampler:
\begin{enumerate}
\item $p(\beta|\ldots)$
\item $p(\sigma_y^2|\ldots)$
\item $p(\mu_\beta|\ldots)$
\item $p(\mySigma_\beta|\ldots)$
\end{enumerate}

\end{frame}



\begin{frame}
\frametitle{Full conditional for $\beta$}

\[ \begin{array}{rl}
p(\beta|\ldots) \propto& p(y|\beta,\sigma_y^2)p(\beta|\mu_\beta,\mySigma_\beta)p(\sigma_y^2,\mu_\beta,\mySigma_\beta) \\
\propto& \prod_{j=1}^J N(y_{j}|X_j\beta_j,\sigma_y^2\mathrm{I}) N(\beta_j|\mu_\beta,\mySigma_\beta)
\end{array} \]
So 
\begin{itemize}
\item The $\beta_j$ are conditionally independent.
\item Each $\beta_j$ is a regression with \emph{informative} prior $N(\mu_\beta,\mySigma_\beta)$.
\end{itemize}
thus 
\[ \beta_j \stackrel{ind}{\sim} N(\hat{\beta}_j, \hat{\mySigma}_{\beta_j}) \]
where 
\[ \begin{array}{rl}
\hat{\mySigma}_{\beta_j} &= \left[ \mySigma_{\beta}^{-1} + \sigma_y^{-2} X_j'X_j \right]^{-1} \\
\hat{\beta}_{j} &= \hat{\mySigma}_{\beta_j}\left[ \mySigma_{\beta}^{-1}\mu_\beta + \sigma_y^{-2} X_j'y_j \right]
\end{array} \]

\end{frame}


\begin{frame}[fragile]
\frametitle{R implementation}

<<>>=
sample_betas = function(data, prior, params) {
  J = data$J
  betaj           = array(NA, dim=c(J,2))
  for (j in 1:J) {
    wh = data$group == j
    X = data$X[wh,]
    Sigma_betaj_hat = solve(prior$Sigma_beta_inv + t(X)%*%X/params$sigma2_y)
    betaj_hat = Sigma_betaj_hat %*% (prior$Sigma_beta_inv %*% prior$mu_beta + X%*%data$y[wh])
    betaj[j,] = rnorm(1, betaj_hat, Sigma_betaj_hat)
  }
  return(betaj)
}
@

\end{frame}


\begin{frame}
\frametitle{Full conditional for $\sigma_y^2$} 

\[ \begin{array}{rl}
p(\sigma_y^2|\ldots) \propto& p(y|\beta,\sigma_y^2)p(\beta|\mu_\beta,\mySigma_\beta)p(\sigma_y^2,\mu_\beta,\mySigma_\beta) \\
\propto& p(y|\beta,\sigma_y^2)p(\sigma_y^2) \\
\propto & (\sigma^2)^{-n/2} e^{-nv/2\sigma_y^2} \frac{1}{\sigma^2} \\
\propto & (\sigma^2)^{-(n/2+1)} e^{-nv/2\sigma_y^2}
\end{array} \]
where 
\[ n = \sum_{j=1}^J n_j \qquad nv = \sum_{j=1}^J \sum_{i=1}^{n_j} (y_{ij} - X_j\beta_j)^2 \]

Thus 
\[ \sigma_y^2 \sim \mbox{Inv-}\chi^2(n, v). \]

\end{frame}


\begin{frame}
\frametitle{Full conditional for $\mySigma_\beta$}
{\small
\[ \begin{array}{rl}
p(\mySigma_\beta|\ldots) 
\propto& p(y|\beta,\sigma_y^2)p(\beta|\mu_\beta,\mySigma_\beta)p(\sigma_y^2,\mu_\beta,\mySigma_\beta) \\
\propto& p(\beta|\mu_\beta,\mySigma_\beta)p(\mySigma_\beta) \\
\propto& \left[ \prod_{j=1}^J |\mySigma|^{-1/2} e^{-\frac{1}{2}(\beta_j-\mu_\beta)\mySigma_\beta^{-1}(\beta_j-\mu_\beta)} \right] \\
&\times |\mySigma|^{-(d+1)}e^{-\frac{1}{2} \mbox{tr}\left(\mySigma_\beta^{-1}\right)} \\
\propto& |\mySigma|^{-(d+1+J/2)} e^{-\frac{1}{2} \mbox{tr}\left(S'\mySigma_\beta^{-1}\right)} 
\end{array} \]
where 
\[ S' = \left( \mathrm{I} + \sum_{j=1}^J (\beta_j-\mu_\beta)(\beta_j-\mu_\beta)^\top \right)^{-1} \]
}

so 
\[ \mySigma_\beta|\ldots \sim IW(d+1+J/2, S'). \]
\end{frame}

\begin{frame}
\frametitle{Full conditional for $\mu_\beta$}
{\small
\[ \begin{array}{rl}
p(\mySigma_\beta|y,\sigma_y^2,\beta,\mu_\beta) 
\propto& p(y|\beta,\sigma_y^2)p(\beta|\mu_\beta,\mySigma_\beta)p(\sigma_y^2,\mu_\beta,\mySigma_\beta) \\
\propto& p(\beta|\mu_\beta,\mySigma_\beta) p(\mu_\beta) \\
\propto& \prod_{j=1}^J |\Sigma_\beta|^{-1/2} e^{-(\beta_j-\mu_beta)^\top \Sigma_\beta^{-1}(\beta_j-\mu_beta)/2} \\
\propto& |\Sigma_\beta|^{-J/2} e^{-\mbox{tr}(\Sigma_\beta^{-1} SS)}
\end{array} \]
where 
\[ SS = \sum_{j=1}^J (\beta_j-\mu_\beta)(\beta_j-\mu_\beta)^\top \]
}

\end{frame}


\end{document}
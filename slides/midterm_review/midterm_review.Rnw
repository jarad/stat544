\documentclass[10pt,handout,xcolor=pdftex,dvipsnames,table]{beamer} % for handouts
%\documentclass[10pt]{beamer}




\usecolortheme[RGB={0,0,144}]{structure}
\usetheme{AnnArbor}\usecolortheme{beaver}
%\usetheme{CambridgeUS}\usecolortheme{crane}

\usepackage{verbatim,xmpmulti,color,multicol,multirow}
\setlength{\unitlength}{\textwidth}  % measure in textwidths
\usepackage[normalem]{ulem}

%\usepackage{beamerthemesplit}
\setbeamertemplate{navigation symbols}{}
%\setbeamercolor{alerted text}{fg=red}
%\setbeamertemplate{block body theorem}{bg=orange}
\setkeys{Gin}{width=0.6\textwidth}

\title{Review}
\author[Jarad Niemi]{Dr. Jarad Niemi}
\institute[Iowa State]{Iowa State University}
\date{\today}

\newcommand{\mG}{\mathrm{\Gamma}}
\newcommand{\I}{\mathrm{I}}
\newcommand{\mySigma}{\mathrm{\Sigma}}

\begin{document}

\frame{\maketitle}

\section{What we have covered}
\frame{\frametitle{What we have covered}
  Chapters
  \begin{itemize}
  \item Probability and inference (Ch 1)
  \item Single-parameter models (Ch 2)
  \item Introduction to multiparameter models (Ch 3)
  \item Hierarchical models (Ch 5)
  \item Introduction to regression models (Ch 14)
  \item Model checking (Ch 6)
  \item JAGS
  \end{itemize}
}

\subsection{Probability and inference}
\frame{\frametitle{Probability and inference (Ch 1)}
  \begin{itemize}[<+->]
  \item Three steps of Bayesian data analysis (Sec 1.1)
    \begin{itemize}
    \item Set up a full probability model: $p(y|\theta)$ and $p(\theta)$
    \item Condition on observed data: $p(\theta|y)$
    \item Evaluate the fit of the model: $p(y^{rep}|y)$
    \end{itemize}
  \item Bayesian inference via Bayes' rule (Sec 1.3)
    \begin{itemize}
    \item Parameter posteriors: $p(\theta|y)\propto p(y|\theta)p(\theta)$
    \item Predictions: $p(\tilde{y}|y) = \int p(\tilde{y}|\theta)p(\theta|y) d\theta$
    \item Model probabilities $p(M|y) \propto p(y|M)p(M)$ where $p(y|M) = \int p(y|\theta,M)p(\theta|M) d\theta$.
    \end{itemize}
  \item Interpreting Bayesian probabilities (Sec 1.5)
    \begin{itemize}
    \item Epistemic probability: my belief
    \item Frequency probability: long run percentage
    \end{itemize}
  \item Computation (Sec 1.9)
    \begin{itemize}
    \item Inference via simulations
    \end{itemize}
  \end{itemize}
}

\subsection{Single-parameter models}
\frame{\frametitle{Single-parameter models (Ch 2)}
  \begin{multicols}{2}
  General
  \begin{itemize}
  \item Priors
    \begin{itemize}
    \item Conjugate (Sec 2.4)
    \item Default - Jeffreys (Sec 2.8)
    \item Weakly informative (Sec 2.9)
    \end{itemize}
  \item Posteriors
    \begin{itemize}
    \item Compromise between data and prior (2.2)
    \item Point estimation 
    \item Credible intervals (Sec 2.3)
    \end{itemize}
  \end{itemize}
  
  \columnbreak
  
  Specific models
  \begin{itemize}
  \item Binomial (Sec 2.1--2.4)
  \item Normal, unknown mean (Sec 2.5)
  \item Normal, unknown variance (Sec 2.6)
  \item Poisson (Sec 2.6)
  \item Exponential (Sec 2.6)
  \item Poisson with exposure (Sec 2.7)
  \end{itemize}
  
  \end{multicols}
}

\subsection{Introduction to multiparameter models}
\frame{\frametitle{Introduction to multiparameter models (Ch 3)}
  \begin{itemize}
  \item Normal model with default prior (Sec 3.2)
  \[ y\stackrel{iid}{\sim} N(\mu,\sigma^2) \quad p(\mu,\sigma^2) \propto 1/\sigma^2 \]
  results in 
  \[ p(\mu,\sigma^2|y) = N(\overline{y}, \sigma^2/n) \mbox{Inv-}\chi^2(n-1,s^2)\]
  where $s^2 = \frac{1}{n} \sum_{i=1}^n (y_i-\overline{y})^2$.
  \item Normal model with conjugate prior (Sec 3.3)
  \[ y\stackrel{iid}{\sim} N(\mu,\sigma^2) \quad \mu|\sigma^2 \sim N(\mu_0, \sigma^2/\kappa_0) \mbox{Inv-}\chi^2(\nu_0,\sigma_0^2) \]
  results in 
  \[ p(\mu,\sigma^2|y) = N\left(\frac{\kappa_0\mu_0+n\overline{y}}{\kappa_0+n}, \frac{\sigma^2}{\kappa_0+n}\right) \mbox{Inv-}\chi^2(\nu_0+n,\sigma_n^2)\]
  where $\sigma_n^2 = \left[\mu_0\sigma_0^2+(n-1)s^2+\frac{\kappa_0n}{\kappa_0+n}(\overline{y}-\mu_0)^2 \right]/(\nu_0+n)$.
%  \item Multinomial model (Sec 3.4)*
  \item Multivariate normal model (Sec 3.5--3.6, HW6)
  \end{itemize}
}

\subsection{Hierarchical models}
\frame{\frametitle{Hierarchical models (Ch 5)}
  \begin{itemize}
  \item Hierarchical model (Ch 5):
  \[ p(\theta,\phi|y) \propto p(y|\theta)p(\theta|phi)p(\phi) \]
  \item Exchangeability (Sec 5.2)
  \[ p(y_1,\ldots,y_n) = p(y_{\pi_1},\ldots,y_{\pi_n}) \]
  \item Hierarchical binomial model (Sec 5.3):
  \[ y_i \stackrel{iid}{\sim} Bin(n_i,\theta_i) \quad \theta_i \stackrel{iid}{\sim} Be(\alpha,\beta) %\quad p(\alpha,\beta) \propto (\alpha+\beta)^{-5/2} 
  \]
  \item Hierarchical Poisson (with exposure) model (HW 5)
  \[ y_i \stackrel{iid}{\sim} Po(x_i\lambda_i) \quad \lambda_i\stackrel{iid}{\sim} Ga(\mu\beta,\beta) \]
  \item Hierarchical normal model (Sec 5.4, HW 7)
  \[ y_{ij} \stackrel{iid}{\sim} N(\mu_j,\sigma_j^2) \quad \mu_j\stackrel{iid}{\sim} N(\eta,\tau^2) \quad \sigma_j^2 \stackrel{iid}{\sim} Ga(\alpha,\beta) \]
  \end{itemize}
}


\subsection{Introduction to regression models}
\frame{\frametitle{Introduction to regression models (Ch 14)}
  \begin{itemize}
  \item Default (classical) regression (Sec 14.2)
  \[ y|\beta,\sigma^2 \sim N(X\beta,\sigma^2\mathrm{I}) \quad p(\beta,\sigma^2) \propto 1/\sigma^2  \]
  results in 
  \[ p(\beta,\sigma^2|y) \sim N(\beta|\hat{\beta}, \sigma^2 [X^\top X]^{-1})\mbox{Inv-}\chi^2(n-k,s^2) \]
  where $\hat{\beta} = [X^\top X]^{-1} X^top y$ and $s^2 = \frac{1}{n-k} (y-X\hat{\beta})^\top (y-X\hat{\beta})$.
  
  \item Regression with known covariance matrix (Sec 14.7)
  \item Regression with conjugate informative prior (Sec 14.8)
  \[ y|\beta,\sigma^2 \sim N(X\beta,\sigma^2\mathrm{I}) \quad \beta|\sigma^2 \sim N(\beta_0,\Sigma_\beta) \quad \sigma^2\sim \mbox{Inv}-\chi^2(\nu_0,\sigma_0^2) \]
  can be thought of as addition data directly on $\beta$ and $\sigma^2$.
  \end{itemize}
}

\subsection{Model checking}
\frame{\frametitle{Model checking (Ch 6)}
  \begin{itemize}
  \item Posterior predictive checking (Sec 6.4)
  \[ p(y^{rep}|y) = \int p(y^{rep}|\theta)p(\theta|y) d\theta \]
  \item Posterior predictive pvalues (Sec 6.3)
  \[ p_B = P(T(y^{rep},\theta)\ge T(y,\theta)|y) \]
  for a test statistic $T(y,\theta)$. 
  \end{itemize}
}


\subsection{JAGS}
\begin{frame}[fragile]
\frametitle{JAGS}
  A non-conjugate normal hierarchical model in JAGS:
  
<<JAGS_example, eval=FALSE, tidy=FALSE>>=
model {
  for (i in 1:n) {
    y[i]     ~ dnorm(theta[group[i]], tau2[group[i]])
    theta[i] ~ dnorm(mu0, 1/sigma0^2)
    tau2[i]  ~ dgamma(nu0, tau20)
  }
  mu0    ~ dnorm(0,0.001)
  sigma0 ~ dunif(0,100) # if number of group is > 4
  nu0    ~ dexp(0.1)
  tau20  ~ dexp(0.1)
}
@
  
This model allows borrowing of information across groups for both the mean and the variance in each group.
\end{frame}

\begin{comment}
\section{Bayesian statistics}
\frame{\frametitle{What makes a Bayesian?}
\small
	\begin{definition}
	A Bayesian statistician is somebody who answers questions based on the probability of quantities she doesn't know conditional on quantities she does. 
	\end{definition}
	
	\vspace{0.2in} \pause
	
	For example, 
	
	\vspace{0.1in} \pause
	
	\begin{tabular}{lll}
	Parameter estimation: & $p(\theta|y)$ & \uncover<7->{$=p(y|\theta)p(\theta)/p(y)$} \pause \\ \\
	Prediction: & $p(\tilde{y}|y)$ & \uncover<8->{$= \int p(\tilde{y}|\theta)p(\theta|y) d\theta$} \pause \\ \\
	Model comparison: & $p(M|y)$ & \uncover<9->{$= p(y|M)p(M)/p(y)$} \pause \\ 
	\end{tabular}
	
	\vspace{0.2in} 
	
	where access to these conditional distributions utilizes Bayes' rule. 
}

\frame{\frametitle{Bayesian inference often match non-Bayesian inference}
	\begin{itemize}[<+->]
	\item Exact (priors $\propto 1$)
		\begin{itemize}
		\item Normal models
		\item Multiple regression
		\item Regularized regression (ridge, LASSO)
		\end{itemize}
	\item Approximate (posterior is approximately normal)
		\begin{itemize}
		\item GLMs
		\item Above when informative priors are dominated by the likelihood
		\end{itemize}
	\end{itemize}
}

\frame{\frametitle{Some things are easier from a Bayesian perspective}
\small
	\begin{itemize}[<+->]
	\item Uncertainty for binomial success probability when
		\begin{itemize}
		\item all successes
		\item all failures
		\end{itemize}
	\item Uncertainty has proper support
		\begin{itemize}
		\item binomial success probability $p$  
		\item MLEs (and CIs) for hierarchical variances
		\end{itemize}
	\item Uncertainty for complicated functions of parameters
		\begin{itemize}
		\item baking example, $-\frac{2BC-DE}{4AB-E^2}$
		\end{itemize}
	\item Dealing with non-standard problems
		\begin{itemize}
		\item Truncating/censoring
		\item Bizarre data models
		\end{itemize}
	\item Small sample sizes
		\begin{itemize}
		\item Bayesians do not rely on data asymptotics
		\end{itemize}
	\end{itemize}
}

\frame{\frametitle{Some things are harder from a Bayesian perspective}
	Non-Bayesians differentiate and Bayesians integrate. 
	
	\vspace{0.2in} \pause
	
	\begin{itemize}[<+->]
	\item Bayesian application relies on BUGS/JAGS/Stan
		\begin{itemize}
		\item convergence issues
		\item parameterization issues
		\item identifiability issues
		\end{itemize}
	\item ``univariate'' Gibbs sampling does not work well for 
		\begin{itemize}
		\item Spatial models
		\item Time series models
		\item Multi-model posteriors
		\end{itemize}
	\end{itemize}
	
	\vspace{0.2in} \pause
	
	Application of Bayesian inference often become computational issues.
}

\section{Errors I've seen}
\frame{\frametitle{Appropriate non-informative priors for variance parameters}
\footnotesize
	There are two types variance parameters:
	\begin{itemize}
	\item Data-level variances
	\item Hierarchical variances
	\end{itemize}
	\pause e.g.
	\[ y_i\stackrel{ind}{\sim} N(\mu_{g[i]},\sigma^2) \qquad \mu_g \stackrel{ind}{\sim} N(\mu,\sigma_g^2) \]
	where $g=1,\ldots,G$. \pause The data-level and hierarchical variances are $\sigma^2$ and $\sigma_g^2$, respectively.
	
	\vspace{0.2in} \pause
	
	According to Gelman's 2006 paper ``Prior distributions for variance parameters in hierarchical models'', the appropriate non-informative priors for variances are
	\begin{itemize}[<+->]
	\item Data-level variances: 
		\begin{itemize}
		\item $p(\sigma^2) \propto \sigma^{-2}$, equivalently $p(\log \sigma)\propto 1$
		\item $p(\sigma^2)\propto \sigma^{-1}$, equivalently $p(\sigma) \propto 1$
		\end{itemize} 
	\item Hierarchical variances: 
		\begin{itemize}
		\item $G\ge 5$: $p(\sigma_g^2)\propto \sigma_g^{-1}$, equivalently $p(\sigma_g) \propto 1$
		\item $G< 5$: $p(\sigma_g) = C^+(0,\tau^2)$ with $\tau$ large
		\end{itemize}
	\end{itemize}
}



\frame{\frametitle{Proper interpretation of credible sets}
	What is wrong with this quote
	\begin{quote}
	Since the 95\% confidence interval for the hierarchical variance parameter  does not
contain 0, I can say there is statistically significant evidence that the relationship between
conversion and the remaining 9 variables benefits from the hierarchical model.
	\end{quote}
	
	\vspace{0.2in} \pause
	
	\begin{itemize}
	\item Bayesians create credible intervals, not confidence intervals.
	\item The credible intervals for variances parameters will never contain zero. 
	\end{itemize}
}

\frame[containsverbatim]{\frametitle{Proper interpretation of credible sets}
\small
	What is wrong with this quote?
\begin{verbatim}
quantile(d.new$x1.opt, c(0.025, 0.5, 0.975)) + mean(d$minutes)
## 2.5% 50% 97.5%
## 34.67 35.82 38.94

quantile(d.new$x2.opt, c(0.025, 0.5, 0.975)) + mean(d$temperature)
## 2.5% 50% 97.5%
## 346.1 352.7 357.6

The best combination of baking time and temperature 
lie in the 95% credible region
[34.7,38.9] x [346, 353].
\end{verbatim}

}

\frame[containsverbatim]{\frametitle{Proper interpretation of credible sets}
\small
	What is wrong with this quote
\begin{verbatim}
quantile(d.new$x1.opt, c(0.025, 0.5, 0.975)) + mean(d$minutes)
## 2.5% 50% 97.5%
## 34.67 35.82 38.94

quantile(d.new$x2.opt, c(0.025, 0.5, 0.975)) + mean(d$temperature)
## 2.5% 50% 97.5%
## 346.1 352.7 357.6

The best combination of baking time and temperature 
lie in the 95% credible region
[34.7,38.9] x [346, 353].
\end{verbatim}

\vspace{0.2in} 

\begin{itemize}
\item A 95\% credible (or confidence) region is not typically the ``product'' of the two individual sets. 
\end{itemize}
}
\end{comment}


\end{document}
\documentclass{beamer}
%\documentclass[handout,xcolor=pdftex,dvipsnames,table]{beamer} % for handouts



\usecolortheme[RGB={0,0,144}]{structure}
\usetheme{AnnArbor}\usecolortheme{beaver}
%\usetheme{CambridgeUS}\usecolortheme{crane}

\usepackage{verbatim,xmpmulti,color,multicol,multirow}
\setlength{\unitlength}{\textwidth}  % measure in textwidths
\usepackage[normalem]{ulem}

%\usepackage{beamerthemesplit}
\setbeamertemplate{navigation symbols}{}
%\setbeamercolor{alerted text}{fg=red}
%\setbeamertemplate{block body theorem}{bg=orange}
\setkeys{Gin}{width=0.6\textwidth}

\title{Review}
\author[Jarad Niemi]{Dr. Jarad Niemi}
\institute[Iowa State]{Iowa State University}
\date{\today}

\newcommand{\mG}{\mathrm{\Gamma}}
\newcommand{\I}{\mathrm{I}}
\newcommand{\mySigma}{\mathrm{\Sigma}}

\begin{document}

\frame{\maketitle}

\section{What we have covered}
\frame{\frametitle{What we have covered}
  Chapters
  \begin{itemize}
  \item[1.] Probability and inference
  \item[2.] Single-parameter models
  \item[3.] Introduction to multiparameter models
  \item[5.] Hierarchical models
  \item[14.] Introduction to regression models
  \item[6.] Model checking
  \end{itemize}
}

\subsection{Probability and inference}
\frame{\frametitle{Probability and inference (Ch 1)}
  \begin{itemize}[<+->]
  \item Three steps of Bayesian data analysis (Sec 1.1)
    \begin{itemize}
    \item Set up a full probability model: $p(y|\theta)$ and $p(\theta)$
    \item Condition on observed data: $p(\theta|y)$
    \item Evaluate the fit of the model: $p(y^{rep}|y)$
    \end{itemize}
  \item Bayesian inference via Bayes' rule (Sec 1.3)
    \begin{itemize}
    \item Parameter posteriors: $p(\theta|y)\propto p(y|\theta)p(\theta)$
    \item Predictions: $p(\tilde{y}|y) = \int p(\tilde{y}|\theta)p(\theta|y) d\theta$
    \item Model probabilities $p(M|y) \propto p(y|M)p(M)$ where $p(y|M) = \int p(y|\theta,M)p(\theta|M) d\theta$.
    \end{itemize}
  \item Interpreting Bayesian probabilities (Sec 1.5)
    \begin{itemize}
    \item Epistemic probability: my belief
    \item Frequency probability: long run percentage
    \end{itemize}
  \item Computation (Sec 1.9)
    \begin{itemize}
    \item Inference via simulations
    \end{itemize}
  \end{itemize}
}

\subsection{Single-parameter models}
\frame{\frametitle{Single-parameter models (Ch 2)}\small
  \begin{multicols}{2}
  General
  \begin{itemize}
  \item Priors
    \begin{itemize}
    \item Conjugate (Sec 2.4)
    \item Default - Jeffreys (Sec 2.8)
    \item Weakly informative (Sec 2.9)
    \end{itemize}
  \item Posteriors
    \begin{itemize}
    \item Compromise between data and prior (2.2)
    \item Point estimation 
    \item Credible intervals (Sec 2.3)
    \end{itemize}
  \end{itemize}
  
  \columnbreak
  
  Specific models
  \begin{itemize}
  \item Binomial (Sec 2.1--2.4)
  \item Normal, unknown mean (Sec 2.5)
  \item Normal, unknown variance (Sec 2.6)
  \item Poisson (Sec 2.6)
  \item Exponential (Sec 2.6)
  \item Poisson with exposure (Sec 2.7)
  \end{itemize}
  
  \end{multicols}
}

\subsection{Introduction to multiparameter models}
\frame{\frametitle{Introduction to multiparameter models (Ch 3)}
  \begin{itemize}
  \item Normal model with $p(\mu,\sigma^2)\propto 1/\sigma^2$ (Sec 3.2)
  \item Normal model with conjugate prior (Sec 3.3)
  \item Multinomial model (Sec 3.4)*
  \item Multivariate normal model (Sec 3.5--3.6)*
  \end{itemize}
  
  \vspace{0.2in} 
  
  * didn't cover
}

\subsection{Hierarchical models}
\frame{\frametitle{Hierarchical models}
  \begin{itemize}
  \item Hierarchical model (Ch 5):
  \[ p(\theta,\phi|y) \propto p(y|\theta)p(\theta|phi)p(\phi) \]
  \item Exchangeability (Sec 5.2)
  \[ p(y_1,\ldots,y_n) = p(y_{\pi_1},\ldots,y_{\pi_n}) \]
  \item Hierarchical binomial model (Sec 5.3):
  \[ y_i \stackrel{iid}{\sim} Bin(n_i,\theta_i) \quad \theta_i \stackrel{iid}{\sim} Be(\alpha,\beta) %\quad p(\alpha,\beta) \propto (\alpha+\beta)^{-5/2} 
  \]
  \item Hierarchical Poisson (with exposure) model (HW 5)
  \[ y_i \stackrel{iid}{\sim} Po(x_i\lambda_i) \quad \lambda_i\stackrel{iid}{\sim} Ga(\mu\beta,\beta) \]
  \item Hierarchical normal model (Sec 5.4, HW 7)
  \[ y_{ij} \stackrel{iid}{\sim} N(\mu_j,\sigma_j^2) \quad \mu_j\stackrel{iid}{\sim} N(\eta,\tau^2) \quad \sigma_j^2 \stackrel{iid}{\sim} Ga(\alpha,\beta) \]
  
  \end{itemize}
}


\subsection{Introduction to regression models}

\subsection{Model checking}

\section{Bayesian statistics}
\frame{\frametitle{What makes a Bayesian?}
\small
	\begin{definition}
	A Bayesian statistician is somebody who answers questions based on the probability of quantities she doesn't know conditional on quantities she does. 
	\end{definition}
	
	\vspace{0.2in} \pause
	
	For example, 
	
	\vspace{0.1in} \pause
	
	\begin{tabular}{lll}
	Parameter estimation: & $p(\theta|y)$ & \uncover<7->{$=p(y|\theta)p(\theta)/p(y)$} \pause \\ \\
	Prediction: & $p(\tilde{y}|y)$ & \uncover<8->{$= \int p(\tilde{y}|\theta)p(\theta|y) d\theta$} \pause \\ \\
	Model comparison: & $p(M|y)$ & \uncover<9->{$= p(y|M)p(M)/p(y)$} \pause \\ 
	\end{tabular}
	
	\vspace{0.2in} 
	
	where access to these conditional distributions utilizes Bayes' rule. 
}

\frame{\frametitle{Bayesian inference often match non-Bayesian inference}
	\begin{itemize}[<+->]
	\item Exact (priors $\propto 1$)
		\begin{itemize}
		\item Normal models
		\item Multiple regression
		\item Regularized regression (ridge, LASSO)
		\end{itemize}
	\item Approximate (posterior is approximately normal)
		\begin{itemize}
		\item GLMs
		\item Above when informative priors are dominated by the likelihood
		\end{itemize}
	\end{itemize}
}

\frame{\frametitle{Some things are easier from a Bayesian perspective}
\small
	\begin{itemize}[<+->]
	\item Uncertainty for binomial success probability when
		\begin{itemize}
		\item all successes
		\item all failures
		\end{itemize}
	\item Uncertainty has proper support
		\begin{itemize}
		\item binomial success probability $p$  
		\item MLEs (and CIs) for hierarchical variances
		\end{itemize}
	\item Uncertainty for complicated functions of parameters
		\begin{itemize}
		\item baking example, $-\frac{2BC-DE}{4AB-E^2}$
		\end{itemize}
	\item Dealing with non-standard problems
		\begin{itemize}
		\item Truncating/censoring
		\item Bizarre data models
		\end{itemize}
	\item Small sample sizes
		\begin{itemize}
		\item Bayesians do not rely on data asymptotics
		\end{itemize}
	\end{itemize}
}

\frame{\frametitle{Some things are harder from a Bayesian perspective}
	Non-Bayesians differentiate and Bayesians integrate. 
	
	\vspace{0.2in} \pause
	
	\begin{itemize}[<+->]
	\item Bayesian application relies on BUGS/JAGS/Stan
		\begin{itemize}
		\item convergence issues
		\item parameterization issues
		\item identifiability issues
		\end{itemize}
	\item ``univariate'' Gibbs sampling does not work well for 
		\begin{itemize}
		\item Spatial models
		\item Time series models
		\item Multi-model posteriors
		\end{itemize}
	\end{itemize}
	
	\vspace{0.2in} \pause
	
	Application of Bayesian inference often become computational issues.
}

\section{Errors I've seen}
\frame{\frametitle{Appropriate non-informative priors for variance parameters}
\footnotesize
	There are two types variance parameters:
	\begin{itemize}
	\item Data-level variances
	\item Hierarchical variances
	\end{itemize}
	\pause e.g.
	\[ y_i\stackrel{ind}{\sim} N(\mu_{g[i]},\sigma^2) \qquad \mu_g \stackrel{ind}{\sim} N(\mu,\sigma_g^2) \]
	where $g=1,\ldots,G$. \pause The data-level and hierarchical variances are $\sigma^2$ and $\sigma_g^2$, respectively.
	
	\vspace{0.2in} \pause
	
	According to Gelman's 2006 paper ``Prior distributions for variance parameters in hierarchical models'', the appropriate non-informative priors for variances are
	\begin{itemize}[<+->]
	\item Data-level variances: 
		\begin{itemize}
		\item $p(\sigma^2) \propto \sigma^{-2}$, equivalently $p(\log \sigma)\propto 1$
		\item $p(\sigma^2)\propto \sigma^{-1}$, equivalently $p(\sigma) \propto 1$
		\end{itemize} 
	\item Hierarchical variances: 
		\begin{itemize}
		\item $G\ge 5$: $p(\sigma_g^2)\propto \sigma_g^{-1}$, equivalently $p(\sigma_g) \propto 1$
		\item $G< 5$: $p(\sigma_g) = C^+(0,\tau^2)$ with $\tau$ large
		\end{itemize}
	\end{itemize}
}



\frame{\frametitle{Proper interpretation of credible sets}
	What is wrong with this quote
	\begin{quote}
	Since the 95\% confidence interval for the hierarchical variance parameter  does not
contain 0, I can say there is statistically significant evidence that the relationship between
conversion and the remaining 9 variables benefits from the hierarchical model.
	\end{quote}
	
	\vspace{0.2in} \pause
	
	\begin{itemize}
	\item Bayesians create credible intervals, not confidence intervals.
	\item The credible intervals for variances parameters will never contain zero. 
	\end{itemize}
}

\frame[containsverbatim]{\frametitle{Proper interpretation of credible sets}
\small
	What is wrong with this quote?
\begin{verbatim}
quantile(d.new$x1.opt, c(0.025, 0.5, 0.975)) + mean(d$minutes)
## 2.5% 50% 97.5%
## 34.67 35.82 38.94

quantile(d.new$x2.opt, c(0.025, 0.5, 0.975)) + mean(d$temperature)
## 2.5% 50% 97.5%
## 346.1 352.7 357.6

The best combination of baking time and temperature 
lie in the 95% credible region
[34.7,38.9] x [346, 353].
\end{verbatim}

}

\frame[containsverbatim]{\frametitle{Proper interpretation of credible sets}
\small
	What is wrong with this quote
\begin{verbatim}
quantile(d.new$x1.opt, c(0.025, 0.5, 0.975)) + mean(d$minutes)
## 2.5% 50% 97.5%
## 34.67 35.82 38.94

quantile(d.new$x2.opt, c(0.025, 0.5, 0.975)) + mean(d$temperature)
## 2.5% 50% 97.5%
## 346.1 352.7 357.6

The best combination of baking time and temperature 
lie in the 95% credible region
[34.7,38.9] x [346, 353].
\end{verbatim}

\vspace{0.2in} 

\begin{itemize}
\item A 95\% credible (or confidence) region is not typically the ``product'' of the two individual sets. 
\end{itemize}
}



\end{document}
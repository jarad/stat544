%\documentclass{beamer}
\documentclass[handout,xcolor=pdftex,dvipsnames,table]{beamer} % for handouts

\usecolortheme[RGB={0,0,144}]{structure}
\usetheme{AnnArbor}\usecolortheme{beaver}
%\usetheme{CambridgeUS}\usecolortheme{crane}

\usepackage{verbatim,xmpmulti,color,multicol,multirow}
\setlength{\unitlength}{\textwidth}  % measure in textwidths
\usepackage[normalem]{ulem}

%\usepackage{beamerthemesplit}
\setbeamertemplate{navigation symbols}{}
%\setbeamercolor{alerted text}{fg=red}
%\setbeamertemplate{block body theorem}{bg=orange}
\setkeys{Gin}{width=0.6\textwidth}

\title{Bayesian parameter estimation}
\author[Jarad Niemi]{Dr. Jarad Niemi}
\institute[Iowa State]{Iowa State University}
\date{\today}

\begin{document}

%\section{Temp??} \begin{comment}

\frame{\titlepage}

\section{Parameter estimation}
\frame{\frametitle{Posterior distribution}
	For point or interval estimation of a parameter $\theta$ in a model $M$ based on data $y$, \pause Bayesian inference is based off  
	\[ p(\theta|y) = \frac{p(y|\theta)p(\theta)}{\alert<7->{p(y)}} \uncover<7->{= \frac{p(y|\theta)p(\theta)}{\alert<7->{\int p(y|\theta)p(\theta) d \theta}}}\uncover<8->{\propto p(y|\theta) p(\theta)} \]
	
	\pause where
	\begin{itemize}[<+->]
	\item $p(\theta)$ is the \alert{prior} distribution for the parameter, 
	\item $p(\theta|y)$ is the \alert{posterior} distribution for the parameter, 
	\item $p(y|\theta)$ is the statistical \alert{model} (or \alert{likelihood}), and
	\item $p(y)$ is the \alert{prior predictive distribution} (or \alert{marginal likelihood}).
	\end{itemize}
}

\frame{\frametitle{Obtaining the posterior}
	The hard way: \pause 
	\begin{enumerate}
	\item Derive $p(y)$.\pause
	\item Derive $p(\theta|y) = p(y|\theta)p(\theta)/p(y)$.\pause
	\end{enumerate}
	
	The easy way:\pause
	\begin{enumerate}
	\item Derive $f(\theta) \propto p(y|\theta)p(\theta)$.\pause
	\item Recognize $f(\theta)$ as the \alert{kernel} of some distribution.\pause
	\end{enumerate}
	
	\begin{definition}
	The \alert{kernel} of a probability density (mass) function is the form of the pdf (pmf) with any terms not involving the random variable omitted. \pause
	\end{definition}
	
	For example, $\theta^{a-1}(1-\theta)^{b-1}$ is the kernel of a beta distribution. 
}


\subsection{the hard way}
\frame{\frametitle{Derive the posterior - the hard way}
	Suppose $Y\sim Bin(n,\theta)$ and $\theta\sim Be(a,b)$, \pause then 
	\[ \begin{array}{ll}
	p(y) &= \int p(y|\theta)p(\theta) d\theta \pause \\
	&= \int {n\choose y} \theta^y(1-\theta)^{n-y}  \frac{\theta^{a-1} (1-\theta)^{b-1}}{\mbox{Beta}(a,b)} d\theta \pause \\
	&= {n\choose y} \frac{1}{\mbox{Beta}(a,b)} \int \theta^{a+y-1} (1-\theta)^{b+n-y-1} d\theta \pause \\
	&= {n\choose y} \frac{\mbox{Beta}(a+y,b+n-y)}{\mbox{Beta}(a,b)} \pause
	\end{array} \]
	and 
	\[ \begin{array}{ll}
	p(\theta|y) &= p(y|\theta)p(\theta) / p(y) \pause \\
	&= {n\choose y} \theta^y(1-\theta)^{n-y}  \frac{\theta^{a-1} (1-\theta)^{b-1}}{\mbox{Beta}(a,b)}  \left/ {n\choose y} \frac{\mbox{Beta}(a+y,b+n-y)}{\mbox{Beta}(a,b)} \right. \pause \\
	&= \frac{\theta^{a+y-1} (1-\theta)^{b+n-y-1}}{\mbox{Beta}(a+y,b+n-y)}  \pause \\
	&= Be(\theta|a+y, b+n-y) 
	\end{array} \]
	
}

\frame{\frametitle{Derive the posterior - the easy way}
	Suppose $Y\sim Bin(n,\theta)$ and $\theta\sim Be(a,b)$, \pause then 
	\[ \begin{array}{ll}
	p(\theta|y) &\propto p(y|\theta)p(\theta) \pause \\
	&\propto \theta^y(1-\theta)^{n-y} \theta^{a-1} (1-\theta)^{b-1} \pause \\
	&= \theta^{a+y-1} (1-\theta)^{b+n-y-1} \pause \\
	&= Be(\theta|a+y, b+n-y) 
	\end{array} \]
}



\frame{\frametitle{Point and interval estimation}
	Nothing inherently Bayesian about obtaining point and interval estimates. 
	
	\vspace{0.2in} \pause
	
	Point estimation requires specifying a loss (or utility) function. 
	
	\vspace{0.2in} \pause
	
	A $100(1-\alpha)\%$ credible interval is any interval in the posterior that contains the parameter with probability $(1-\alpha)$.
}

\subsection{Point estimation}
\frame{\frametitle{Point estimation}
	Define utility function $U\!\left(\theta,\hat{\theta}\right)$ \pause where 
	\begin{itemize}
	\item $\theta$ is the parameter of interest \pause
	\item $\hat{\theta}=\hat{\theta}(y)$ is the estimator of $\theta$.  \pause
	\end{itemize}
	or loss function $L\!\left(\theta,\hat{\theta}\right)=-U\!\left(\theta,\hat{\theta}\right)$.
	
	\vspace{0.1in} \pause 
	
	Find the estimator that maximizes the expected utility:
	\[ \hat{\theta}_{Bayes} = \mbox{argmax}_{\hat{\theta}} \,E\left[ U\!\left(\theta,\hat{\theta}\right) \right] \] 
	\pause or minimizes expected loss. 
	
	\vspace{0.1in} \pause
	
	Common estimators: \pause
	\begin{itemize}
	\item Mean: $\hat{\theta}_{Bayes} = E[\theta|y]$ minimizes $U\!\left(\theta,\hat{\theta}\right) = \left(\theta-\hat{\theta}\right)^2$ \pause
	\item Median: $\int_{\hat{\theta}_{Bayes}}^\infty p(\theta|y) d\theta = \frac{1}{2}$ minimizes $U\!\left(\theta,\hat{\theta}\right) = \left|\theta-\hat{\theta}\right|$ \pause
	\item Mode: $\hat{\theta}_{Bayes} = \mbox{argmax}_\theta \, p(\theta|y)$ minimizes $U\!\left(\theta,\hat{\theta}\right) = \mathrm{I}\left(\theta = \hat{\theta}\right)$  \pause also called \alert{maximum a posterior (MAP)}
	\end{itemize}
}


\frame{\frametitle{Mean minimizes squared-error loss}

	\begin{theorem}
	The mean minimizes expected squared-error loss. \pause
	\end{theorem}
	
	\begin{proof}
	Suppose $L\!\left(\theta,\hat{\theta}\right) = \left(\theta-\hat{\theta}\right)^2 \pause = \theta^2 -2\theta\hat{\theta} + \hat{\theta}^2$,
	\pause then 
	
	\[ \begin{array}{ll}
	E_\theta \left[L\!\left(\theta,\hat{\theta}\right)\right] & = E_\theta\left[\theta^2\right] -2\hat{\theta}E_\theta[\theta] + \hat{\theta}^2 \pause \\ \\ 
	\frac{d}{d\hat{\theta}} E\left[L\!\left(\theta,\hat{\theta}\right)\right] &= -2E_\theta[\theta] + 2\hat{\theta} \pause \stackrel{set}{=} 0 \pause \implies \hat{\theta} = E_\theta[\theta] \pause \\ \\
	\frac{d^2}{d\hat{\theta}^2} E\left[L\!\left(\theta,\hat{\theta}\right)\right] &= 2 \pause
	\end{array} \]
	So $\hat{\theta} = E_\theta[\theta]$ minimizes expected squared-error loss. 
	\end{proof}
}




\subsection{Interval estimation}
\frame{\frametitle{Interval estimation}
	\begin{definition}
	A $100(1-a)\%$ \alert{credible interval} is any interval (L,U) such that 
	\[ 1-a = \int_L^U p(\theta|y) d\theta. \]
	\end{definition}
	
	\vspace{0.2in} \pause 
	
	Some typical intervals \pause are 
	\begin{itemize}
	\item Equal-tailed: $a/2 = \int_{-\infty}^L p(\theta|y) d\theta = \int_U^\infty p(\theta|y) d\theta$ \pause
	\item One-sided: either $L=-\infty$ or $U=\infty$ \pause
	\item \alert{Highest posterior density (HPD)}: $p(L|y) = p(U|y)$
	\end{itemize}
}



\begin{comment}

\subsection{Priors}
\frame{\frametitle{Priors}
	\begin{definition}
	A prior $p(\theta)$ is \alert{conjugate} if for $p(\theta)\in \mathcal{P}$ and $p(y|\theta)\in \mathcal{F}$, $p(\theta|y)\in \mathcal{P}$ where $\mathcal{F}$ and $\mathcal{P}$ are families of distributions.
	\end{definition}
	
	\vspace{0.1in} \pause  
	
	For example, the beta distribution ($\mathcal{P}$) is conjugate to the binomial distribution with unknown probability of success ($\mathcal{F}$) \pause since 
	\[ \theta \sim \alert{\mbox{Be}}(a,b) \qquad\mbox{and}\qquad \theta|y \sim \alert{\mbox{Be}}(a+y,b+n-y). \] 
	
	\pause
	
	\begin{definition}
	A \alert{natural} conjugate prior is a conjugate prior that has the same functional form as the likelihood.
	\end{definition}
	
	\vspace{0.1in} \pause 
	
	For example, the beta distribution is a natural conjugate prior since 
	\[ p(\theta) \propto \theta^{a-1}(1-\theta)^{b-1} \qquad \mbox{and} \qquad L(\theta) \propto \theta^y(1-\theta)^{n-y}. \]
}


\frame{\frametitle{Discrete priors are conjugate}
	\begin{theorem}
	Discrete priors are conjugate. \pause
	\end{theorem}
	
	\begin{proof}
	Suppose $p(\theta)$ is discrete, \pause i.e. 
	\[ P(\theta=\theta_i) = p_i \pause \qquad \sum_{i=1}^\mathrm{I} p_i = 1 \]
	\pause and $p(y|\theta)$ is the model. \pause Then, $P(\theta=\theta_i) = p_i'$ 
	is the posterior \pause with 
	\[ p_i' = \frac{p_i p(y|\theta_i)}{\sum_{j=1}^\mathrm{I} p_j p(y|\theta_j)} \propto p_i p(y|\theta_i). \]
	\end{proof}
}

\frame{\frametitle{Mixtures of natural conjugate priors are conjugate}
	\begin{theorem}
	Mixtures of natural conjugate priors are conjugate. \pause
	\end{theorem}
	
	\begin{proof}
	Let 
	\[ \theta\sim \sum_{i=1}^\mathrm{I} p_i p_i(\theta) \pause \qquad \sum_{i=1}^\mathrm{I} p_i=1\] \pause and $p_i(y) = \int p(y|\theta) p_i(\theta) d\theta$, \pause then 
	\[ \theta|y \sim \sum_{i=1}^\mathrm{I} p_i' p_i(\theta|y) \pause \qquad p_i' \propto p_i p_i(y) \]
	\pause where $p_i(\theta|y)$ is the posterior if $p_i(\theta)$ was the prior. 
	\end{proof}
}


\subsection{Objective priors}
\frame{\frametitle{Objective (default?) priors}
	Can we always use $p(\theta)\propto 1$? 
	
	\vspace{0.2in} \pause 
	
	Suppose we use $\phi = \log(\theta/[1-\theta])$, the log odds as our parameter, \pause and set $p(\phi) \propto 1$, \pause then the implied prior on $\theta$ \pause is 
	
	\[ \begin{array}{ll}
	p_\theta(\theta) \propto & 1 \left| \frac{d}{d\theta} \log(\theta/[1-\theta]) \right| \pause \\
	&= \frac{1-\theta}{\theta} \left[ \frac{1}{1-\theta} + \frac{\theta}{[1-\theta]^2} \right] \pause \\	
	&= \frac{1-\theta}{\theta} \left[ \frac{[1-\theta]+\theta}{[1-\theta]^2} \right] \pause \\
	&= \theta^{-1}[1-\theta]^{-1} \pause \\
	\end{array} \]
	a Be(0,0) if that were a proper distribution. 
}

\frame{\frametitle{Jeffreys Prior}
	\begin{definition}
	\alert{Jeffreys prior} is a prior that is invariant to parameterization \pause and is obtained  via 
	\[ p(\theta) \propto \sqrt{\mbox{det}\,  \mathcal{I}(\theta)} \]
	\pause where $\mathcal{I}(\theta)$ is the Fisher information. 
	\end{definition}
	
	\vspace{0.2in} \pause
	
	For example, for a binomial distribution $\mathcal{I}(\theta)=\frac{n}{\theta[1-\theta]}$, \pause so 
	\[ p(\theta) \propto \theta^{-1/2}(1-\theta)^{-1/2} =  \theta^{1/2-1}(1-\theta)^{1/2-1} \]
	\pause a Be(1/2,1/2) distribution. 
}

\subsection{Summary}
\frame{\frametitle{Summary}
	\begin{itemize}[<+->]
	\item Bayesian parameter estimation
		\begin{itemize}
		\item $p(\theta|y) \propto p(y|\theta)p(\theta)$
		\item Point estimation: mean, median, MAP
		\item Interval estimation: equal-tailed, one-sided, HPD
		\end{itemize}
	\item Priors
		\begin{itemize}
		\item Conjugate
		\item Jeffreys prior
		\end{itemize}
	\end{itemize}
}

\end{comment}

\end{document}


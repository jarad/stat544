\documentclass[handout,xcolor=pdftex,dvipsnames,table]{beamer} % for handouts
%\documentclass{beamer}

\usecolortheme[RGB={0,0,144}]{structure}
\usetheme{AnnArbor}\usecolortheme{beaver}
%\usetheme{CambridgeUS}\usecolortheme{crane}

\usepackage{verbatim,xmpmulti,color,multicol,multirow}
\setlength{\unitlength}{\textwidth}  % measure in textwidths
\usepackage[normalem]{ulem}

%\usepackage{beamerthemesplit}
\setbeamertemplate{navigation symbols}{}
%\setbeamercolor{alerted text}{fg=red}
%\setbeamertemplate{block body theorem}{bg=orange}
\setkeys{Gin}{width=0.6\textwidth}

\title{Introduction to Bayesian computation}
\author[Jarad Niemi]{Dr. Jarad Niemi}
\institute[Iowa State]{Iowa State University}
\date{\today}

\newcommand{\I}{\mathrm{I}}

\begin{document}

%\section{Temp??} \begin{comment}

<<chunk_options, echo=FALSE, message=FALSE>>=
library(knitr) # only needed so the following command does not fail when sourcing R code
opts_chunk$set(fig.width=6, fig.height=5, out.width='.8\\linewidth', fig.align='center', size='tiny')
##############################################
# L13 - Introduction to Bayesian computation #
##############################################
library(reshape2)
library(plyr)
library(ggplot2)
library(rjags)
@

\frame{\maketitle}

\section{Introduction to Bayesian computation}
\frame{\frametitle{Notation}
  \begin{itemize}
  \item Target distribution: $p(\theta|y)$
  \item Unnormalized target distribution: $q(\theta|y)$, i.e.
  \[ p(\theta|y) = \frac{p(y|\theta)p(\theta)}{p(y)} \propto q(\theta|y) \]
  \item Proposal distribution: $g(\theta)$ which may depend on $y$
  \end{itemize}
}

\subsection{Numerical integration}
\frame{\frametitle{Numerical integration}
  \begin{itemize}[<+->]
  \item Simulation methods:
  \[ E[h(\theta)|y] = \int h(\theta)p(\theta|y) d\theta \approx \frac{1}{S} \sum_{S=1}^S h\left(\theta^s\right) \]
	where 
    \begin{itemize}
    \item $\theta^s \stackrel{iid}{\sim} p(\theta|y)$ \pause 
    \item and we have SLLN and CLT.
    \end{itemize}
  \item Deterministic methods:
  \[ E[h(\theta)|y] = \int h(\theta)p(\theta|y) d\theta \approx \frac{1}{S} \sum_{S=1}^S w_s h\left(\theta^s\right)p(\theta^s|y) \]
  where 
    \begin{itemize}
    \item $\theta^s$ are selected points, 
    \item $w_s$ is the volume of space represented by the point $\theta^s$, and
    \item the error can be bounded. 
    \end{itemize}
  
	\end{itemize}
}

\subsection{Example: Normal-Cauchy model}
\frame{\frametitle{Example: Normal-Cauchy model}
  Let $Y\stackrel{iid}{\sim} N(\theta,1)$ with $\theta\sim Ca(0,1)$. \pause The posterior is 
  \[ p(\theta|y) \propto p(y|\theta)p(\theta) \propto \frac{\exp(-(y-\theta)^2/2)}{1+\theta^2} \]
  \pause which is not a known distribution. \pause We might be interested in
  \begin{enumerate}
  \item  normalizing this posterior, i.e. calculating
  \[ p(y) = \int p(y|\theta)p(\theta) d\theta \]
  \item or in calculating the posterior mean, i.e. 
  \[ E[\theta|y] = \int \theta p(\theta|y) d\theta. \]
  \end{enumerate}
}

\begin{frame}[fragile]
\frametitle{Example:Normal-Cauchy model}
<<normal_cauchy_posterior>>=
q = function(theta,y,log=FALSE) {
  out = -(y-theta)^2/2-log(1+theta^2)
  if (log) return(out)
  return(exp(out))
}

# Find marginal likelihood for y=0
w = 0.1
x = seq(-5,5,by=w)
(py = sum(q(x,0)*w))
integrate(function(x) q(x,0), -Inf, Inf)
@
\end{frame}

\begin{frame}[fragile]
\frametitle{Example:Normal-Cauchy model}
<<normal_cauchy_posterior_plot, fig.width=8>>=
curve(q(x,0), -5, 5, n=1001)
points(x,rep(0,length(x)), cex=0.5, pch=19)
segments(x,0,x,q(x,0))
@
\end{frame}


\begin{frame}[fragile]
\frametitle{Example:Normal-Cauchy model}
<<normal_cauchy_posterior_mean>>=
# Calculate marginal likelihood, p(y)
py = function(y,log=FALSE) {
  int = integrate(function(x) q(x,y,log=FALSE),-Inf,Inf)
  if (int$message!="OK") {
    warning(paste("Could not compute marginal likelihood for y=", y, "\n"))
    return(NA)
  }
  int$value 
}

# Find posterior expectation when y=0
post_expectation = function(y) {
  py = py(y)
  int = integrate(function(x) x*q(x,y)/py, -Inf, Inf)
    if (int$message!="OK") {
    warning(paste("Could not compute posterior expectation for y=", y, "\n"))
    return(NA)
  }
  int$value
}

d = data.frame(y=seq(-5,5, by=0.1))
res = ddply(d, .(y), function(x) { data.frame(post_expectation=post_expectation(x$y))})
res$prior = "Cauchy"

# If theta ~ N(0,1)
res = merge(res, data.frame(y=res$y, post_expectation=res$y/2, prior="normal"), all=TRUE)
@
\end{frame}



\begin{frame}[fragile]
\frametitle{Example:Normal-Cauchy model}
<<normal_cauchy_posterior_mean_plot, fig.width=8>>=
ggplot(res,aes(y,post_expectation,color=prior))+geom_line()+geom_abline(intercept=0,slope=1)
@
\end{frame}


\subsection{Gridding}
\frame{\frametitle{Monte Carlo approximation via gridding}
  Rather than approximate $p(y)$ and then $E[\theta|y]$ via deterministic gridding, we can use the grid as a discrete approximation to the posterior, i.e. 
  \[ p(\theta|y) \approx \sum_{i=1}^N p_i \delta_{\theta_i}(\theta) \qquad p_i = \frac{p(\theta_i|y)}{\sum_{j=1}^N p(\theta_j|y)}  \]
  \pause where $\delta_{\theta_i}(\theta)$ is the Dirac delta function, i.e. 
  \[ \delta_{\theta_i}(\theta) = 0 \,\forall\, \theta\ne \theta_i \qquad \int \delta_{\theta_i}(\theta) d\theta=1. \]
  This discrete approximation to $p(\theta|y)$ can be used to approximate the expectation $E[h(\theta)|y]$ deterministically or via simulation, i.e. 
  \[ E[h(\theta)|y]\approx \sum_{i=1}^N p_i h(\theta_i) \qquad E[h(\theta)|y]\approx \frac{1}{S} \sum_{s=1}^S h(\theta^s) \] 
  where $\theta^s \sim \sum_{i=1}^N p_i \delta_{\theta_i}(\theta)$ (with replacement).
}

\begin{frame}[fragile]
\frametitle{Example:Normal-Cauchy model}
<<normal_cauchy_posterior_mean_grid, fig.width=8>>=
# Small number of grid locations
x = seq(-5,5,length=101)
sum(q(x,0)*x)
mean(x[sample(length(x),prob=q(x,0),replace=TRUE)])


# Large number of grid locations
x = seq(-5,5,length=1000001)
sum(q(x,0)*x) # Takes ~1 sec
# But small MC sample
mean(x[sample(length(x),101,prob=q(x,0),replace=TRUE)])
@
\end{frame}


\begin{comment}
\subsection{Inverse CDF method}
\frame{\frametitle{Inverse cumulative distribution function}
  \begin{definition}
	The \alert{cumulative distribution function} of a random variable $X$ is defined by 
	\[ F_X(x) = P_X(X\le x) \qquad\mbox{for all }x. \]
	\end{definition}
	\pause e.g. $X\sim Exp(1)$, then $F_X(x) = \int_0^x e^{-a} da = 1-e^{-x}$.
	
	\vspace{0.2in} \pause
	
	\begin{definition}
	For any non-decreasing function F on $\mathbb{R}$, \pause the \alert{generalized inverse of $F$}, $F^{-}$, is the function defined by 
	\[ F^-(u) = \inf\{x: F(x)\ge u\}. \]
	\end{definition}
	\pause e.g. $F^-(u) = -\log(1-u)$ for the exponential example.
}

\frame{\frametitle{Inverse CDF method}
	Suppose you want to sample $X\sim f(x)$ and you have access to the inverse cdf of X, $F^-$, then 
	\begin{lemma}
	If $U\sim Unif(0,1)$, then $X=F^-(U)$ is a simulation from $f(x)$. 
	\end{lemma}
	
	\vspace{0.2in} \pause
	
	For example, to sample $X\sim Exp(1)$, 
	\begin{enumerate}[\,1.]
	\item Sample $U\sim Unif(0,1)$.
	\item Set $X = -\log(1-U)$, or $X=-\log(U)$.  
	\end{enumerate}
}

\begin{frame}[fragile]
$X \sim Exp(1)$
<<exponential, message=FALSE, fig.width=8>>=
n.reps = 1e4
d = data.frame(x = -log(runif(n.reps)))
ggplot(d, aes(x=x))+geom_histogram(aes(y=..density..))+stat_function(fun=dexp, color="red")
@
\end{frame}

\frame{\frametitle{Sampling from a univariate truncated normal}
	Suppose you wish to sample from $X\sim N(\mu,\sigma^2)\mathrm{I}(a<X<b)$, i.e. a normal random variable with (untruncated) mean $\mu$ and variance $\sigma^2$, but truncated to the interval $(a,b)$. \pause Let $\mathrm{\Phi}$ be the standard normal cdf. \pause 
	
	\begin{enumerate}[\,1.]
	\item Calculate endpoints $p_a = \mathrm{\Phi}([a-\mu]/\sigma)$ and $p_b = \mathrm{\Phi}([b-\mu]/\sigma)$. \pause
	\item Sample $U\sim Unif(p_a,p_b)$. \pause
	\item Set $X=\sigma \mathrm{\Phi}^{-}(U) + \mu$. \pause
	\end{enumerate}	
}

\begin{frame}[fragile]
$X\sim N(5,9)\mathrm{I}(1\le X \le 6)$
<<truncated_normal, message=FALSE, fig.width=8>>=
d = data.frame(x= 5+3*qnorm(runif(n.reps, pnorm((1-5)/3), pnorm((6-5)/3))))
dtnorm = function(x,mu=5,sigma=3,a=1,b=6) {
  dnorm(x,mu,sigma)/diff(pnorm(c(a,b),mu,sigma))
} 
ggplot(d, aes(x=x))+geom_histogram(aes(y=..density..))+stat_function(fun=dtnorm, color="red")
@
\end{frame}



\subsection{Gridding}



\end{comment}



\subsection{Rejection sampling}
\frame{\frametitle{Rejection sampling}
  Suppose you wish to obtain samples $\theta\sim p(\theta|y)$, \pause rejection sampling performs the following \pause 
	\begin{enumerate}[\,1.]
	\item Sample a proposal $\theta^*\sim g(\theta)$ \pause and $U\sim Unif(0,1)$. \pause 
	\item Accept $\theta=\theta^*$ as a draw from $p(\theta|y)$ if $U\le p(\theta^*|y)/Mg(\theta^*)$\pause, otherwise return to step 1. \pause
	\end{enumerate}
	where $M$ satisfies $M\, g(\theta)\ge p(\theta|y)$ for all $\theta$. 
	
	\vspace{0.2in} \pause
	
	\begin{itemize}[<+->]
	\item For a given proposal distribution $g(\theta)$, the optimal $M$ is $M=\sup_\theta p(\theta|y)/g(\theta)$. 
	\item The probability of acceptance is $1/M$.
	\item $p(\theta|y)$ only needs to be known up to a normalizing constant.
	\end{itemize}
  \pause The accept-reject idea is to create an envelope, $M\, g(\theta)$, above $p(\theta|y)$.
}

\begin{frame}
\frametitle{Example: Normal-Cauchy model}
  If $Y \sim N(\theta,1)$ and $\theta\sim Ca(0,1)$, then 
  \[ p(\theta|y) \propto \frac{\frac{1}{\sqrt{2\pi}} e^{-(y-\theta)^2/2}}{\frac{1}{\pi}(1+\theta^2)} \]
  for all $\theta$. 
  
  \vspace{0.2in} \pause 
  
  In this case, a proposal distribution is a standard normal, i.e. 
  \[ g(\theta)=\frac{1}{\sqrt{2\pi}}  e^{-(\theta-y)^2/2} \]
  with 
  \[ \frac{p(\theta|y)}{g(\theta)} = \frac{1}{\frac{1}{\pi}(1+\theta^2)} \le \frac{1}{\pi} \]
\end{frame}

\begin{frame}[fragile]
\frametitle{Example: Normal-Cauchy model}
<<rejection_sampling, eval=FALSE>>=
M = 1/pi
g = function(theta,y,log=FALSE) dnorm(theta,y,log=log)
q = function(theta,y,log=FALSE) {
  d = dnorm(y,theta,log=TRUE)+dcauchy(theta,log=TRUE)
  if (log) return(d)
  return(exp(d))
}

n = 1e2
d = data.frame(x = rnorm(n), u = runif(n))
d$u_scaled = d$u*M*g(d$x,0)
d$accept = d$u_scaled < q(d$x,0)

xx = seq(-3,3,by=0.01)
dd = data.frame(x=xx,
                y=c(M*g(xx,0),q(xx,0)),
                distribution = c(rep("envelope",length(xx)),
                                 rep("target",length(xx))))

gg = ggplot(d, aes(x=x,y=u_scaled,col=accept)) +geom_point()
clrs = unique(ggplot_build(gg)$data[[1]]$colour)
gg + stat_function(fun=function(x) M*g(x,0), col=clrs[2]) +
  stat_function(fun=function(x) q(x,0), col=clrs[1]) + labs(x="sample",y=expression(paste("u M g(",theta,")")))
mean(d$accept)
@
\end{frame}


\begin{frame}[fragile]
\frametitle{Example: Normal-Cauchy model}
<<rejection_sampling_plot, echo=FALSE>>=
<<rejection_sampling>>
@
\end{frame}


\frame{\frametitle{Heavy-tailed proposals}
  Suppose our target is a Cauchy and our (proposed) proposal is a standard normal, then
  \[ \frac{p(\theta|y)}{g(\theta)} = \frac{\frac{1}{\pi}(1+\theta^2)}{\frac{1}{\sqrt{2\pi}}e^{-\theta^2/2}}  \]
  \pause as $\theta\to \infty$,
  \[
  \frac{\frac{1}{\pi}(1+\theta^2)}{\frac{1}{\sqrt{2\pi}}e^{-\theta^2/2}} \pause 
  \ge  \frac{\frac{1}{\pi}(1+\theta^2)}{\frac{1}{\sqrt{2\pi}}} \pause 
  \stackrel{\theta\to\infty}{\longrightarrow} \infty  \]
  \pause Thus, there is no value $M$ such that $M \, g(\theta) \ge p(\theta|y)$. 
  
  \vspace{0.2in} \pause
  
  Bottom line: the condition $M\, g(\theta)\ge p(\theta|y)$ requires the proposal to have tails at least as thick (heavy) as the target.
}


\subsection{Importance sampling}
\frame{\frametitle{Importance sampling}
  Notice that 
  \[ E[h(\theta)|y] = \int h(\theta) p(\theta|y) d\theta = \int h(\theta) \frac{p(\theta|y)}{g(\theta)} g(\theta) d\theta \]
  where $g(\theta)$ is a proposal distribution\pause, so that we approximate the expectation via 
  \[ E[h(\theta)|y] \approx \frac{1}{S} \sum_{s=1}^S w(\theta^s) h(\theta^s) \]
  where $\theta^s \stackrel{iid}{\sim} g(\theta)$ and 
  \[ w(\theta^s) = \frac{p(\theta^s|y)}{g(\theta^s)} \]
  is known as the importance weight. 
}

\frame{\frametitle{Importance sampling}
  If the target distribution is known only up to a proportionality constant, then 
  \[ E[h(\theta)|y] = \frac{\int h(\theta) q(\theta|y) d\theta}{\int q(\theta|y) d\theta} = \frac{\int h(\theta) \frac{p(\theta|y)}{g(\theta)} g(\theta) d\theta}{\int \frac{p(\theta|y)}{g(\theta)} g(\theta) d\theta} \]
  where $g(\theta)$ is a proposal distribution\pause, so that we approximate the expectation via 
  \[ E[h(\theta)|y] \approx \frac{\frac{1}{S} \sum_{s=1}^S w(\theta^s) h(\theta^s)}{\frac{1}{S} \sum_{s=1}^S w(\theta^s)} = \sum_{i=1}^S \tilde{w}(\theta^s) h(\theta^s) \]
  where $\theta^s \stackrel{iid}{\sim} g(\theta)$ and 
  \[ \tilde{w}(\theta^s) = \frac{w(\theta^s)}{\sum_{j=1}^S w(\theta^j)} \]
  is the \alert{normalized} importance weight. 
}

\begin{frame}[fragile]
<<importance_sampling, fig.width=8>>=
n = 1000
theta = rnorm(n)
weight = q(theta,0)/g(theta,0)
qplot(x=theta,y=weight)+geom_point()
@
\end{frame}

\begin{frame}[fragile]
<<importance_sampling_mean, message=FALSE, fig.width=8>>=
library(weights)
mean(weight*theta/sum(weight))
wtd.hist(theta, 100, prob=TRUE, weight=weight)
@
\end{frame}

\frame{\frametitle{Heavy-tailed proposals}
  Although any proposal can be used for importance sampling, only proposals with heavy tails relative to the target will be efficient. 
  
  \vspace{0.2in} \pause
  
  For example, suppose our target is a standard Cauchy and our proposal is a standard normal, the weights are 
  \[ w(\theta^s) = \frac{q(\theta^s|y)}{g(\theta^s)} = \frac{\frac{1}{\pi}(1+\theta^2)}{\frac{1}{\sqrt{2\pi}}e^{-\theta^2/2}} \]
  For $\theta^s\stackrel{iid}{\sim} N(0,1)$, the weight for the largest $|\theta^s|$ will dominate the others.
}

\begin{frame}[fragile]
<<importance_sampling_weights, fig.width=8>>=
set.seed(16)
theta = rnorm(100)
weight = dcauchy(theta)/dnorm(theta)
weight = weight/sum(weight)
qplot(theta,weight,geom="point")+ylim(0,max(weight))
@
\end{frame}

\begin{frame}[fragile]
\frametitle{Effective sample size}
We can get a measure of how efficient the sample is by computing the effective sample size, i.e. how many independent unweighted draws do we effectively have:
\[ S_{eff} = \frac{1}{\sum_{s=1}^S (\tilde{w}(\theta^s))^2} \]
<<effective_size>>=
1/sum(weight^2)
@
\end{frame}

\begin{frame}[fragile]
\frametitle{Effective sample size}
<<cumulative_effective_size, fig.width=10, cache=TRUE>>=
set.seed(5)
theta = rnorm(10000)
lweight = dcauchy(theta,log=TRUE)-dnorm(theta,log=TRUE)
cumulative_ess = length(lweight)
for (i in 1:length(lweight)) {
  lw = lweight[1:i]
  w = exp(lw-max(lw))
  w = w/sum(w)
  cumulative_ess[i] = 1/sum(w^2)
}
qplot(x=1:length(cumulative_ess), y=cumulative_ess, geom="line")+labs(x="Number of samples", y="Effective sample size")
@
\end{frame}

\begin{frame}[fragile]
\frametitle{Resampling}
If an unweighted sample is desired, sample with replacement with probability equal to the normalized weights.

<<resampling, eval=FALSE>>=
n = 10000
theta = rcauchy(n) # proposal
lweight = dnorm(theta, log=TRUE)-dcauchy(theta, log=TRUE)
lweight = lweight-max(lweight)

# resampling
new_theta = theta[sample(n, n, replace=TRUE, prob=exp(lweight))]
hist(new_theta, 100, prob=TRUE)
curve(dnorm, add=TRUE, col="red", lwd=2)
@

\end{frame}


\begin{frame}[fragile]
\frametitle{Creating an unweighted sample from a weighted sample}
<<resampling_plot, echo=FALSE>>=
<<resampling>>
@
\end{frame}


\begin{comment}
\begin{frame}[fragile]
\frametitle{The idea}
<<accept_reject, eval=FALSE>>=
f = function(x) dnorm(x)/(1-pnorm(5))
q = function(x) dexp(x-5)
M = f(5)/q(5)

plot.pt = function(x) {
  u = runif(length(x))
  points(x,u*M*q(x), 
         pch=ifelse(M*q(x)*u<f(x), 19, 4),
         col=ifelse(M*q(x)*u<f(x), "red", "blue"))
}

par(mfrow=c(1,2), mar=c(5,4,4,2)+0.1)
curve(M*dexp(x-5), 5, 6, col="blue", lwd=2, xlab="x", ylab="u*M*q(x)", ylim=c(0,M), main=paste("M=",round(M,2)))
curve(f, col="red",lwd=2, add=T)
legend("topright",c("f(x)","M*q(x)"), col=c("red","blue"), lwd=2)
plot.pt(5+qexp(runif(1e2,0,pexp(1))))



M = f(5)/q(5)*10

plot.pt = function(x) {
  u = runif(length(x))
  points(x,u*M*q(x), 
         pch=ifelse(M*q(x)*u<f(x), 19, 4),
         col=ifelse(M*q(x)*u<f(x), "red", "blue"))
}

curve(M*dexp(x-5), 5, 6, col="blue", lwd=2, xlab="x", ylab="u*M*q(x)", ylim=c(0,M), main=paste("M=",round(M,2)))
curve(f, col="red",lwd=2, add=T)
legend("topright",c("f(x)","M*q(x)"), col=c("red","blue"), lwd=2)
plot.pt(5+qexp(runif(1e2,0,pexp(1))))
@
\end{frame}

\begin{frame}[fragile]
\frametitle{The idea}
<<accept_reject_plot, echo=FALSE>>=
<<accept_reject>>
@
\end{frame}

\frame{\frametitle{Simulating extreme events}
	Suppose you are interested in simulating $X\sim N(0,1)\mathrm{I}(X>5)$. \pause We know that 
 	\[ f(x) = \frac{\frac{1}{\sqrt{2\pi}} \exp(-x^2/2)}{1-\mathrm{\Phi}(5)}. \]
	\pause We can use a shifted exponential distribution as a proposal, i.e. $X^* = 5+Y$ where $Y\sim Exp(1)$. \pause We calculate
	{\small
	\[ M = \sup_x \frac{f(x)}{q(x)} \pause = \frac{\frac{1}{\sqrt{2\pi}}}{1-\mathrm{\Phi}(5)} \sup_x \frac{\exp(-x^2/2)}{\exp(-[x-5])} \pause = \frac{\frac{1}{\sqrt{2\pi}}}{1-\mathrm{\Phi}(5)} \exp(-5^2/2) \pause \approx 5.19 \]
	}
}

\begin{frame}[fragile]
<<accept_reject2, eval=FALSE>>=
M = f(5)/q(5)
ar = function() {
  x = 0
  while(x<5) {
    u = runif(1)
    x = rexp(1)
    x = ifelse(u<f(x)/(M*q(x)), x, 0)
  }
  return(x)
}

r = rdply(1e3, ar)
ggplot(r, aes(x=V1))+
  geom_histogram(aes(y=..density..), binwidth=.05)+
  stat_function(fun=f, col="red")+
  stat_function(fun=function(x) M*dexp(x-5), col="blue")
@
\end{frame}

\begin{frame}[fragile]
<<accept_reject2_plot, echo=FALSE, cache=TRUE>>=
<<accept_reject2>>
@
\end{frame}

\frame{\frametitle{Unknown normalizing constant}
	Suppose you are interested in simulating $X\sim N(0,1)\mathrm{I}(X>5)$. \pause We know that 
	\[ f(x)\propto f_2(x) = \exp(-x^2/2). \]
	\pause We calculate
	\[ M_2 = \sup_x \frac{f_2(x)}{q(x)} \pause = \sup_x \frac{\exp(-x^2/2)}{\exp(-[x-5])} \pause= \exp(-5^2/2) \pause \approx 3.73\times 10^{-6}  \]
  \pause We can still draw $X^*\sim q(x)$ and accept with probability $f_2(x)/M_2 q(x)$ \pause since
	\[ M = \frac{\frac{1}{\sqrt{2\pi}}}{1-\mathrm{\Phi}(5)} M_2 \pause \implies \frac{f_2(x)}{M_2} = \frac{f(x)}{M}  \] 
	for all $x$. \pause But $M_2$ does not relate to the acceptance probability.
}

\section{Summary}
\frame{\frametitle{Summary}
	The accept-reject method is a way of obtaining samples from $f(x)$ \pause when
	\begin{itemize}
	\item the inverse cdf cannot be computed \pause or is expensive to compute \pause and
	\item when $f(x)$ can be evaluated at least up to a normalizing constant. \pause 
	\end{itemize}
	Based on 
	\begin{itemize}
	\item a draw from a proposal $q(x)$, \pause 
	\item a constant $M$ such that $f(x) \le M q(x)$\pause, and
	\item a uniform draw.
	\end{itemize}
}
\end{comment}






\end{document}

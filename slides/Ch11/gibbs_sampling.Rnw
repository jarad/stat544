\documentclass[handout,xcolor=pdftex,dvipsnames,table]{beamer} % for handouts
%\documentclass{beamer}

\usecolortheme[RGB={0,0,144}]{structure}
\usetheme{AnnArbor}\usecolortheme{beaver}
%\usetheme{CambridgeUS}\usecolortheme{crane}

\usepackage{verbatim,xmpmulti,color,multicol,multirow,tikz,animate}
\setlength{\unitlength}{\textwidth}  % measure in textwidths
\usepackage[normalem]{ulem}

%\usepackage{beamerthemesplit}
\setbeamertemplate{navigation symbols}{}
%\setbeamercolor{alerted text}{fg=red}
%\setbeamertemplate{block body theorem}{bg=orange}
\setkeys{Gin}{width=0.6\textwidth}


\title[Metropolis-Hastings]{Gibbs sampling}
\author[Jarad Niemi]{Dr. Jarad Niemi}
\institute[Iowa State]{Iowa State University}
\date{\today}

\newcommand{\I}{\mathrm{I}}

\begin{document}

%\section{Temp??} \begin{comment}

<<chunk_options, echo=FALSE, message=FALSE>>=
library(knitr) # only needed so the following command does not fail when sourcing R code
opts_chunk$set(fig.width=6, fig.height=5, out.width='.8\\linewidth', fig.align='center', size='tiny')
##############################################
# Markov chain Monte Carlo                   #
##############################################
library(reshape2)
library(plyr)
library(ggplot2)
library(mvtnorm)
set.seed(2)
@

\frame{\maketitle}

\section{Markov chain Monte Carlo}
\begin{frame}
\frametitle{Markov chain construction}

Suppose we wish to simulate from $p(\theta|y)$, but cannot do so directly. 

\vspace{0.2in} 

Construct a Markov chain with stationary distributon $p(\theta|y)$ and run it long enough that the samples are approximately from $p(\theta|y)$. 

\end{frame}


\section{Gibbs sampling}
\frame{\frametitle{Gibbs sampling}
  Suppose our target distribution is $p(\theta|y)$ with $\theta=(\theta_1,\theta_2)$  \pause and we can sample from $p\left(\theta_1|\theta_2\right)$ and $p\left(\theta_2|\theta_1\right)$\pause. Beginning with an initial value $\left(\theta_1^0, \theta_2^0\right)$\pause, the \alert{Gibbs sampler} is 
	\begin{enumerate}[\quad1.]
	\item sample $\theta_1^{t}\sim p\left(\theta_1|\theta_2^{t-1}\right)$ \pause and then 
	\item sample $\theta_2^{t}\sim p\left(\theta_2|\theta_1^{t}\right)$. 
	\end{enumerate}
	
	\vspace{0.2in} \pause
	
	Notes:
	\begin{itemize}[<+->]
  \item The sequence $\{\theta^0,\theta^1,\ldots,\}$ is a Markov chain with transition distribution 
  \[ p(\theta^{t}|\theta^{t-1}) = p(\theta_2^{t}|\theta_1^{t})p(\theta_1^{t}|\theta_2^{t-1}). \]
	\item $\theta^{t} \stackrel{d}{\rightarrow} \theta$ where $\theta \sim p\left(\theta|y\right)$.  
%	\item 
%	\[  \frac{1}{J} \sum_{j=1}^J h\left(\theta^{t}\right) \rightarrow E_p[h(\theta)] = \int_{\Theta} h(\theta) f(\theta) d\theta  \]
	\end{itemize}
}

\subsection{Bivariate normal example}
\frame{\frametitle{Bivariate normal example}
\small
	Let our target be 
	\[ \theta \sim N_2(0,\mathrm\Sigma) \pause \qquad \mathrm{\Sigma} = \left[ \begin{array}{cc} 1 & \rho \\ \rho & 1 \end{array} \right].\]
	\pause Then 
	\[ \begin{array}{rl}
	\theta_1|\theta_2 &\sim \pause N\left(\rho\theta_2, [1-\rho^2]\right) \pause \\
	\theta_2|\theta_1 &\sim \pause N\left(\rho\theta_1, [1-\rho^2]\right) 
	\end{array} \]
	are the conditional distributions. 
	
	\vspace{0.2in} \pause
	
	Assuming initial value $\left(\theta_1^{(0)}, \theta_2^{(0)}\right)$, the Gibbs sampler proceeds as follows:
	\[ \begin{array}{ccc}
	\mbox{Iteration} & \mbox{Sample }\theta_1 & \mbox{Sample }\theta_2 \pause \\
	\hline 
	1 & \theta_1\sim N\left(\rho\theta_2^{(0)}, [1-\rho^2]\right) &\theta_2 \sim N\left(\rho\theta_1^{(1)}, [1-\rho^2]\right) \pause \\
	& \vdots & \\
	k & \theta_1\sim N\left(\rho\theta_2^{(k-1)}, [1-\rho^2]\right) &\theta_2 \sim N\left(\rho\theta_1^{(k)}, [1-\rho^2]\right) \pause \\
	& \vdots &
	\end{array} \]	
}

\begin{frame}[fragile]
\frametitle{Bivariate normal with $\rho=0.9$}
<<bivariate_normal_mcmc>>=
gibbs_bivariate_normal = function(x0, n_points, rho) {
  x = matrix(x0, nrow=n_points, ncol=2, byrow=TRUE)
  v = sqrt(1-rho^2)
  for (i in 2:n_points) {
    x[i,1] = rnorm(1, rho*x[i-1,2], v)
    x[i,2] = rnorm(1, rho*x[i  ,1], v)
  }
  return(x)
}

set.seed(1)
x = gibbs_bivariate_normal(c(-3,3), n<-20, rho=rho<-0.9)
@
\end{frame}

<<bivariate_normal, eval=FALSE, echo=FALSE>>=
# Create plot
n.out = 101
xx <- seq(-3, 3, length=n.out)
grid <- expand.grid(x=xx, y=xx)
Sigma = diag(rep(.1,2))+rho
like <- matrix(apply(grid, 1, function(x) dmvnorm(x,sigma=Sigma)),n.out,n.out)

contour(xx, xx, like, drawlabels=F, nlevels=10, xlim=c(-3,3), ylim=c(-3,3), 
        xlab=expression(theta[1]), ylab=expression(theta[2]))
points(x[1,1],x[1,2],col="red", pch=19)

for (i in 2:nrow(x)) {
  plot.new()
  contour(xx, xx, like, drawlabels=F, nlevels=10, xlim=c(-3,3), ylim=c(-3,3), 
        xlab=expression(theta[1]), ylab=expression(theta[2]))
  
  for (j in 2:i) {
    points(x[j-1,1],x[j-1,2],col="red", pch=19)
    segments(x[j-1,1], x[j-1,2], x[j,1], x[j-1,2],col="gray")
    segments(x[j  ,1], x[j-1,2], x[j,1], x[j  ,2],col="gray")
  }
}
@

\begin{frame}[fragile]
<<bivariate_normal_animation, fig.show='animate', echo=FALSE>>=
<<bivariate_normal>>
@
\end{frame}


\subsection{Extending the Gibbs sampler}
\frame{\frametitle{$K$-component Gibbs sampler}
	Suppose $\theta = (\theta_1,\ldots,\theta_K)$\pause, then the following Gibbs sampler can be constructed \pause
	\[ \begin{array}{rl}
	\theta_1^{t} &\sim p\left(\theta_1|\theta_2^{t-1},\ldots, \theta_K^{t-1}\right) \pause \\ \\
	\theta_2^{t} &\sim p\left(\theta_2|\theta_1^{t}, \theta_3^{t-1},\ldots, \theta_K^{t-1}\right) \pause\\
	&\vdots \\
	\theta_k^{t} &\sim p\left(\theta_k|\theta_1^{t}\ldots,\theta_{k-1}^{t}, \theta_{k+1}^{t-1},\ldots, \theta_K^{t-1}\right) \\
	&\vdots \pause\\
	\theta_K^{t} &\sim p\left(\theta_K|\theta_1^{t},\ldots, \theta_{K-1}^{t}\right) \pause
	\end{array} \]
	The distributions above are called the \alert{full conditional distributions}.
}


\section{Metropolis-within-Gibbs}
\frame{\frametitle{Metropolis-within-Gibbs}
  We have discussed two Markov chain approaches to sampling from target distributions:
	\begin{itemize}
	\item Metropolis-Hastings algorithm
	\item Gibbs sampling
	\end{itemize}
	
	\vspace{0.2in} \pause
	
	Gibbs sampling assumed we can sample from $p(\theta_k|\theta_{-k})$ for all $k$\pause, but what if we cannot sample from all of these full conditional distributions? \pause For those $p(\theta_k|\theta_{-k})$ that cannot be sampled directly, a Metropolis-Hastings step can be substituted. \pause All the convergence results for Gibbs sampling and Metropolis-Hastings algorithm still hold. 
}


\begin{frame}[fragile]
\frametitle{Bivariate normal with $\rho=0.9$}
<<gibbs_and_metropolis_mcmc>>=
gibbs_and_metropolis = function(x0, n_points, rho) {
  x = matrix(x0, nrow=n_points, ncol=2, byrow=TRUE)
  v = sqrt(1-rho^2)
  for (i in 2:n_points) {
    x[i,1] = rnorm(1, rho*x[i-1,2], v)
    
    # Now do a random-walk Metropolis step
    x_prop = rnorm(1, x[i-1,2], 2.4*v) # optimal proposal variance
    logr   = dnorm(1, rho*x_prop,   v, log=TRUE) - 
             dnorm(1, rho*x[i-1,2], v, log=TRUE)
    x[i,2] = ifelse(log(runif(1))<logr, x_prop, x[i-1,2])
  }
  return(x)
}

x = gibbs_and_metropolis(c(-3,3), n, rho)
@
\end{frame}

<<gibbs_and_metropolis, eval=FALSE, echo=FALSE>>=
# Create plot
contour(xx, xx, like, drawlabels=F, nlevels=10, xlim=c(-3,3), ylim=c(-3,3), 
        xlab=expression(theta[1]), ylab=expression(theta[2]))
points(x[1,1],x[1,2],col="red", pch=19)

for (i in 2:nrow(x)) {
  plot.new()
  contour(xx, xx, like, drawlabels=F, nlevels=10, xlim=c(-3,3), ylim=c(-3,3), 
        xlab=expression(theta[1]), ylab=expression(theta[2]))
  
  for (j in 2:i) {
    points(x[j-1,1],x[j-1,2],col="red", pch=19)
    segments(x[j-1,1], x[j-1,2], x[j,1], x[j-1,2],col="gray")
    segments(x[j  ,1], x[j-1,2], x[j,1], x[j  ,2],col="gray")
  }
}
@

\begin{frame}[fragile]
<<gibbs_and_metropolis_animation, fig.show='animate', echo=FALSE>>=
<<gibbs_and_metropolis>>
@
\end{frame}



\section{Slice sampling}

This post is a demonstration of slice sampling. The goal is to sample from a target distribution f(x). A slice variable, u, is added where the joint density is p(u,x) = I(0<u<f(x)) which maintains the marginal distribution for x. A [Gibbs sampler](http://jarad.me/stat544/2013/03/gibbs-sampling/) is constructed with the following steps

1. u|x ~ Unif(0,f(x))
1. x|u ~ Unif over the set A where A={x:u<f(x)}.

The rest of this post considers situations where A is relatively easy to find, e.g. monotone functions f(x) over the support of x and unimodal functions. 


Here is a generic function (okay, it probably isn't that generic, but it works for the purpose below) to implement slice sampling. It takes a number of draws n, an initial x, the target density, and a function A that returns the interval A, so only unimodal targets supported. The function returns the samples x and u (for demonstration).
```{r}
# Slice sampler
slice = function(n,init_x,target,A) {
  u = x = rep(NA,n)
  x[1] = init_x
  u[1] = runif(1,0,target(x[1])) # This never actually gets used

  for (i in 2:n) {
    u[i] = runif(1,0,target(x[i-1]))
    endpoints = A(u[i],x[i-1]) # The second argument is used in the second example
    x[i] = runif(1, endpoints[1],endpoints[2])
  }
  return(list(x=x,u=u))
}
```

In this first example, the target distribution is Exp(1). The set A is just the interval (0,-log(u)) where u is the current value of the slice variable. 

```{r}
# Exponential example
set.seed(6)
A = function(u,x=NA) c(0,-log(u))
res = slice(10, 0.1, dexp, A)
x = res$x
u = res$u
```

Here is a demonstration of a single step of the slice sampler. Given a coordinate (u,x), the sampler first draws from u|x which is equivalent to slicing the density vertically at x and drawing uniformly on the y-axis from the interval where u<f(x) and then draws x|u which is equivalent ot slicing the density horizontally at u and drawing uniformly on the x-axis from the interval where u<f(x). 

```{r}
i = 2
curve(dexp,0,2,ylim=c(0,1))
points(x[i],u[i],pch=19)
segments(x[i], 0, x[i], dexp(x[i]), col="gray")
arrows(x[i], u[i], x[i], u[i+1], length=0.1)
points(x[i],u[i+1])
segments(A(u[i+1])[1],u[i+1],A(u[i+1])[2],u[i+1],col="gray")
arrows(x[i], u[i+1], x[i+1], u[i+1], length=0.1)
points(x[i+1],u[i+1],pch=19)
# This code was used step-by-step in class.
```

Here are 9 steps, 10 samples including the intial, of the slice sampler:

```{r}
# Nine steps
curve(dexp,0,3,ylim=c(0,1))
for (i in 1:9) {
  points(x[i],u[i],pch=19, cex=0.5)
  segments(x[i], u[i], x[i], u[i+1], col="gray")
  segments(x[i], u[i+1], x[i+1], u[i+1], col="gray")
}
points(x[i+1],u[i+1],pch=19, cex=0.5)
```

And this is comparing the marginal draws for x to the truth. 

```{r}
hist(slice(1e4, 0.1, dexp, A)$x, freq=F, 100)
curve(dexp, add=TRUE)
```

Now I turn to a standard normal distribution where we pretend we cannot invert the density and therefore need another approach. The approach is going to be to use numerical methods to find the interval A (so, again assuming a unimodal target). The magic happens below where the `uniroot` function is used to find the endpoints of the interval.

```{r}
A = function(u,xx) {
  c(uniroot(function(x) dnorm(x)-u, c(-10^10,xx))$root, 
    uniroot(function(x) dnorm(x)-u, c(xx, 10^10))$root)
}
```

Run the sampler for a standard normal target. 

```{r}
set.seed(6)
res = slice(10, 0.1, dnorm, A)
x = res$x
u = res$u
```

This is what one step looks like.

```{r} 
# One step
i = 4
curve(dnorm,-3,3,ylim=c(0,0.5))
points(x[i],u[i],pch=19)
segments(x[i], 0, x[i], dnorm(x[i]), col="gray")
arrows(x[i], u[i], x[i], u[i+1], length=0.1)
points(x[i],u[i+1])
segments(A(u[i+1],x[i])[1],u[i+1],A(u[i+1],x[i])[2],u[i+1],col="gray")
arrows(x[i], u[i+1], x[i+1], u[i+1], length=0.1)
points(x[i+1],u[i+1],pch=19)
```

Or 9 steps:
```{r}
# Nine steps
curve(dnorm,-3,3,ylim=c(0,0.5))
for (i in 1:9) {
  points(x[i],u[i],pch=19, cex=0.5)
  segments(x[i], u[i], x[i], u[i+1], col="gray")
  segments(x[i], u[i+1], x[i+1], u[i+1], col="gray")
}
points(x[i+1],u[i+1],pch=19, cex=0.5)
```

And the fit to the target distribution.
```{r}
hist(slice(1e4, 0.1, dnorm, A)$x, freq=F, 100)
curve(dnorm, add=TRUE)
```

## Using an unnormalized density

Slice sampling can also be performed on the unnormalized density. 

```{r}
####
# Standard normal with unnormalized density
set.seed(6)
target = function(x) exp(-x^2/2) # The normalizing factor is 1/sqrt(2pi)
A = function(u,x) {
  x = sqrt(-2*log(u))
  return(c(-x,x))
}

hist(slice(1e4, 0.1, target, A)$x, freq=F, 100)
curve(dnorm, add=TRUE)
```

## Learning the truncation points

Here is an alternative version of the slice sampler that samples from a distribution other than uniform. The main purpose of introducing this is in Bayesian inference where the target distribution is the posterior, p(x|y) \propto p(y|x) p(x), and the augmentation is p(u,x)\propto p(x) I(0<u<p(y|x)). So the conditional for x|u is a truncated version of the prior. 

The truncation points are determined by u<p(y|x) and the truncation points are not assumed to be known. Instead, they are learned through proposed values that are rejected. So, initially we sample from p(x)I(-Inf<0<Inf) (if x has support on the whole real line) and then adjust to p(x)I(LB<x<UB) where LB and UB are updated depending on whether the proposed value that was rejected was larger or smaller than the current value for x. If the proposed value is larger than the current, then UB becomes the proposed value and if it is smaller than LB becomes the proposed value.

```{r}
# Slice sampler that learns endpoints
slice2 = function(n,init_x,like,qprior) {
  u = x = rep(NA,n)
  x[1] = init_x
  u[1] = runif(1,0, like(x[1]))

  for (i in 2:n) {
    u[i] = runif(1,0, like(x[i-1]))
    success = FALSE
    endpoints = 0:1
    while (!success) {
      up = runif(1, endpoints[1], endpoints[2])
      x[i] = qprior(up)
      if (u[i]<like(x[i])) {
        success=TRUE
      } else
      {
        # Updated endpoints when proposed value is rejected
        if (x[i]>x[i-1]) endpoints[2] = up
        if (x[i]<x[i-1]) endpoints[1] = up
      }
    }
    
  }
  return(list(x=x,u=u))
}
```

This is run on the model y|x ~ N(x,1) and x~N(0,1) and y=1 is observed. The true posterior is N(y/2,1/2).

```{r}
res = slice2(1e4, 0.1, function(x) dnorm(x,1), qnorm)

hist(res$x,freq=F,100)
curve(dnorm(x,0.5,sqrt(1/2)), add=TRUE)
```


\end{document}
